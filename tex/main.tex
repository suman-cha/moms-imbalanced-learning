\documentclass[preprint,review, 12t]{elsarticle}

% Encoding & core packages
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{longtable}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}


\urlstyle{same}

\newcommand{\mP}{\mathbb{P}}
\newcommand{\mE}{\mathbb{E}}
\newcommand{\given}{\,|\,}
\newcommand{\iid}{ \overset{\mathrm{i.i.d.}}{\sim}}

% Journal
\journal{Information Sciences}

%% ESWA: APA style via Elsevier model5-names + authoryear
\bibliographystyle{model5-names}\biboptions{authoryear}

% Hyperref should be loaded near the end
\usepackage[hidelinks]{hyperref}

\begin{document}
\begin{frontmatter}

\title{Learning Majority-to-Minority Transformations with MMD and Triplet Loss for Imbalanced Classification}

\author[label1]{Suman Cha}
\ead{oldrain123@yonsei.ac.kr}

\author[label1]{Hyunjoong Kim\corref{cor1}}
\ead{hkim@yonsei.ac.kr}

\cortext[cor1]{Corresponding author.}
\address[label1]{Department of Statistics and Data Science, Yonsei University, Seoul, South Korea}

\begin{abstract}
Class imbalance in supervised classification often degrades model performance by biasing predictions toward the majority class, particularly in critical applications such as medical diagnosis and fraud detection. Traditional oversampling techniques, including SMOTE and its variants, generate synthetic minority samples via local interpolation but fail to capture global data distributions in high-dimensional spaces. Deep generative models based on GANs offer richer distribution modeling yet suffer from training instability and mode collapse under severe imbalance. To overcome these limitations, we introduce an oversampling framework that learns a parametric transformation to map majority samples into the minority distribution. Our approach minimizes the maximum mean discrepancy (MMD) between transformed and true minority samples for global alignment, and incorporates a triplet loss regularizer to enforce boundary awareness by guiding synthesized samples toward challenging borderline regions. We evaluate our method on 29 synthetic and real-world datasets, demonstrating consistent improvements over classical and generative baselines in AUROC, G-mean, F1-score, and MCC. These results confirm the robustness, computational efficiency, and practical utility of the proposed framework for imbalanced classification tasks.
\end{abstract}

\begin{keyword}
majority-to-minority transformation \sep maximum mean discrepancy \sep triplet loss \sep imbalanced classification \sep generative models \sep data augmentation
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:introduction}

Class imbalance is a pervasive challenge in supervised classification where minority class instances are substantially outnumbered by majority class samples \citep{he2009learning, krawczyk2016learning}. This problem manifests in critical domains including medical diagnosis \citep{roy2024learning}, fraud detection \citep{awoyemi2017credit}, and anomaly detection \citep{chandola2009anomaly}, where poor recognition of minority cases can yield severe consequences \citep{japkowicz2002class}. Aside from impairing predictive performance, class imbalance fosters algorithmic bias, leading to inaccurate representations and distorted decision-making \citep{binns2018fairness, mehrabi2021survey}. 

Data-level resampling, particularly oversampling the minority class, remains a prominent strategy for mitigating imbalance \citep{drummond2003c4}. Traditional methods such as the Synthetic Minority Oversampling Technique (SMOTE) \citep{chawla2002smote} and its variants including Borderline-SMOTE \citep{han2005borderline} and ADASYN \citep{he2008adasyn} synthesize minority samples via local interpolations among existing minority instances. While simple and computationally efficient, these methods struggle to capture the global data distribution, especially in high-dimensional feature spaces \citep{bellinger2016beyond, alkhawaldeh2023challenges}, often producing samples that are noisy or situated far from decision boundaries. 

\begin{sloppypar}
More recently, deep generative models have been explored to overcome these shortcomings. 
Methods based on Generative Adversarial Networks (GANs) \citep{goodfellow2014generative} and 
Variational Autoencoders (VAEs) \citep{kingma2013auto} offer richer minority sample generation 
reflecting complex data distributions. For instance, Generative Adversarial Minority Oversampling (GAMO) 
\citep{mullick2019generative} introduces an auxiliary classifier to encourage boundary-aware sample synthesis. 
BAGAN \citep{mariani2018bagan} further alleviates imbalance by jointly training on all classes and employing an 
autoencoder\-based initialization to learn class\-conditioned latent representa\-tions, enabling the generator to 
produce diverse and high\-quality minority examples. SMOTified-GAN \citep{sharma2022smotified} incorporates SMOTE 
principles into GANs to address minority class sparsity, while VAE-based approaches \citep{zhang2018over} use the 
latent generative capacities to produce diverse minority samples. Despite these advances, deep generative model-based 
methods remain challenging due to training instability, mode collapse, and hyperparameter sensitivity, particularly 
under extreme imbalance settings \citep{salimans2016improved}.
\end{sloppypar}

Another vein of research has focused on exploiting abundant majority data to guide synthetic minority generation. MWMOTE \citep{barua2012mwmote} prioritizes majority class regions for oversampling through local instance weighting. The M2m framework \citep{kim2020m2m} introduces a classifier-dependent transformation from majority samples to the minority domain, but relies on a pretrained classifier to define the decision boundary. Majority-Guided VAE (MGVAE) \citep{ai2023generative} transfers majority diversity via structured latent priors but confines guidance to latent space without explicit alignment to input-space decision boundaries. 

Motivated by these limitations, we propose a novel oversampling framework that learns an explicit parametric mapping $f_\theta$ to transform majority samples into the minority distribution. Our method minimizes the Maximum Mean Discrepancy (MMD) \citep{gretton2012kernel} between transformed and true minority samples to ensure global distributional alignment. To guarantee that generated samples reside in informative regions near the decision boundary, we introduce a triplet loss regularizer \citep{weinberger2009distance, schroff2015facenet}. We define triplets inspired by the danger set concept in Borderline-SMOTE \citep{han2005borderline}. This regularizer explicitly pushes transformed samples towards borderline minority points while maintaining separation from majority clusters.

The key contributions of this paper are twofold. First, we propose a novel oversampling framework that learns a parametric majority-to-minority transformation by minimizing the MMD. Second, we introduce a boundary-aware triplet regularizer that promotes sample generation near decision boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our method consistently outperforms existing oversampling techniques across various imbalance settings.

The remainder of this paper is organized as follows. Section~\ref{sec:related_works} reviews related literature. Section~\ref{sec:proposed_method} details the proposed approach. Sections~\ref{sec:simulation_studies} and \ref{sec:real_data_exp} present empirical results from simulations and real data. Finally, Section~\ref{sec:conclusion} offers concluding remarks and outlines avenues for future research.

\section{Related Works}
\label{sec:related_works}
Synthetic sample generation to address class imbalance has attracted significant attention in machine learning. This section reviews relevant prior work emphasizing three interconnected research paradigms: kernel-based distributional alignment via MMD, boundary-aware geometric regularization employing triplet loss, and integrated oversampling strategies that simultaneously address both global distributional properties and local boundary sensitivity. 

Integral probability metrics, particularly MMD, have become foundational tools across tasks including two-sample testing \citep{gretton2012kernel, liu2020learning, kubler2020learning}, domain adaptation \citep{long2015learning, tzeng2017adversarial}, and deep generative modeling \citep{dziugaite2015training, li2015generative}. Within generative modeling, MMD offers a kernel-based discrepancy measure that promotes alignment between generated and target distributions. Generative Moment Matching Networks (GMMN) \citep{li2015generative} pioneered this approach by replacing the discriminator in GANs with a two-sample test using MMD, which facilitates stable generator training without adversarial objectives. To improve model expressiveness and training efficiency, subsequent work such as MMD-GAN \citep{li2017mmd} introduced kernel adversarial learning. \textcolor{blue}{Recent advances \citep{hertrich2024generative} have demonstrated that sliced MMD flows with Riesz kernels achieve improved computational efficiency and theoretical convergence guarantees, enabling scalable training of generative models through tractable approximations of the MMD objective.} Despite these advancements, existing MMD-based methods have been applied primarily to image synthesis and unsupervised generation tasks, with limited focus on class imbalance scenarios. Using the statistical consistency and stable convergence of MMD \citep{binkowski2018demystifying}, we adopt it as a principled distributional loss to circumvent adversarial training pitfalls—such as mode collapse and gradient instability \citep{salimans2016improved}.

Geometric regularization frameworks based on triplet loss \citep{weinberger2009distance, schroff2015facenet} have demonstrated substantial success in metric learning \citep{hoffer2015deep, sohn2016improved} and discriminative representation learning for applications including face recognition \citep{schroff2015facenet} and image retrieval \citep{hermans2017defense}. In imbalanced classification, triplet loss has primarily been used to enhance feature-space separability at the classifier level \citep{kang2019decoupling, li2020overcoming}, with its potential to guide synthetic sample generation remaining largely unexplored. \textcolor{blue}{Recent work \citep{mildenberger2025tale} has shown that adapting supervised contrastive learning frameworks for imbalanced datasets requires careful attention to representation space geometry, particularly in managing class-specific feature distributions and decision boundaries.} Traditional oversampling methods such as Borderline-SMOTE \citep{han2005borderline} and MWMOTE \citep{barua2012mwmote} incorporate heuristic boundary-awareness, but lack formal geometric regularization mechanisms that explicitly encode relative distances among synthetic, minority, and majority samples.

Only a small number of recent studies have attempted to integrate both distributional alignment and geometric placement in oversampling. The M2m framework \citep{kim2020m2m} learns a classifier-guided mapping from majority to minority domains but omits explicit distributional discrepancy criteria and relies on the performance of the classifier. Majority-guided VAE (MGVAE) \citep{ai2023generative} incorporates majority class structure through latent priors, operating primarily in the latent representation space without explicit control of decision boundary alignment. Likewise, adversarial generative oversampling models including BAGAN \citep{mariani2018bagan} and SMOTified-GAN \citep{sharma2022smotified} focus on sample fidelity and global feature alignment but do not impose boundary-aware regularization. 

\textcolor{blue}{A comprehensive review of oversampling techniques \citep{yang2024review} systematically categorizes existing approaches across geometric interpolation, clustering-based, and generative modeling paradigms, highlighting that effective oversampling for multi-class imbalanced datasets requires balancing global distributional fidelity with local class boundary preservation—a challenge that remains inadequately addressed by existing methods.}

In summary, while MMD-based distribution matching and triplet-loss geometric regularization have been independently applied in various machine learning settings, their combined use for imbalanced classification remains underexplored. To our knowledge, this paper presents the first transport-map--based framework that integrates MMD-based distribution alignment with boundary-aware triplet regularization for synthetic minority sample generation. Beyond advancing oversampling methods for tabular data, our approach establishes a theoretical foundation adaptable to more complex modalities.

\section{Proposed Method}
\label{sec:proposed_method}

We introduce a transport-map--based oversampling framework that learns a parametric transformation to project majority samples into the minority class domain. This framework combines global distribution alignment via MMD with local geometric regularization employing a triplet loss to ensure that generated samples populate informative boundary regions. 

\subsection{Problem Formulation}

Consider a labeled dataset $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{n}$, where $\mathbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{0,1\}$, where $0$ denotes the majority class and $1$ the minority class. We partition $\mathcal{D}$ into majority $\mathcal{D}_{\text{maj}} = \{\mathbf{x}_i \given y_i = 0\}$ of size $n_{\text{maj}}$ and minority $\mathcal{D}_{\text{min}} = \{\mathbf{x}_i \given y_i = 1\}$ of size $n_{\text{min}}$, where $n_{\text{maj}} \gg n_{\text{min}}$. Our objective is to learn a mapping $f_\theta : \mathbb{R}^d \to \mathbb{R}^d$ such that the distribution of transformed majority samples $f_\theta(\mathcal{D}_{\text{maj}})$ approximates the true minority distribution. \noindent Formally, we solve

\begin{equation*}
\theta^{\ast} = \arg\min_\theta \mathcal{L}_{\text{total}}(f_\theta(\mathcal{D}_\text{maj}), \mathcal{D}_\text{min}),     
\end{equation*}
with the total loss decomposed as  
\begin{equation} \label{eq: total_obj}
    \mathcal{L}_{\text{total}}(f_\theta(\mathcal{D}_{\text{maj}}), \mathcal{D}_{\text{min}}) = \mathcal{L}_{\text{MMD}}(f_\theta(\mathcal{D}_{\text{maj}}), \mathcal{D}_{\text{min}}) + \lambda \mathcal{L}_{\text{triplet}}(f_\theta(\mathcal{D}_{\text{maj}}), \mathcal{D}_{\text{min}}),
\end{equation}
where $\lambda > 0$ trades off global distributional alignment and boundary-aware regularization. After training the mapping, synthetic minority samples are generated by applying the learned transformation to majority instances as $\tilde{\mathbf{x}} = f_{\theta^{\ast}}(\mathbf{x})$ for $\mathbf{x} \in \mathcal{D}_{\text{maj}}$.  

\vspace{1em}
\noindent 
{\bf Remark. } In our study, we implemented the transformation map $f_\theta : \mathbb{R}^d \rightarrow \mathbb{R}^d$ as a \emph{transformation network}—a deep feedforward neural network with residual skip connections—which empirically facilitated stable training and expressive feature transformations. The transformation network is trained end-to-end via the supervised distribution-matching objective in Equation~\eqref{eq: total_obj}, minimizing the combined MMD and triplet loss. However, the proposed framework is not restricted to this architectural choice. The essential requirement is merely that $f_\theta$ be a measurable mapping from $\mathbb{R}^d$ to $\mathbb{R}^d$, as demanded by the distribution-matching criterion. Thus, in principle, $f_\theta$ may be instantiated as any sufficiently expressive parametric or nonparametric function—such as feedforward networks, invertible flows, kernel regressors, or even shallow linear maps—provided they admit optimization via the loss in Equation~\eqref{eq: total_obj}. This generality enables practitioners to adapt the transformation network to the structural properties or inductive biases most appropriate for a given application or data modality, without altering the fundamental learning objective. Complete implementation details, including network architecture specifications and training procedures, are publicly available at \url{https://github.com/suman-cha/moms-imbalanced-learning}.


\subsection{Maximum Mean Discrepancy}
To align the distribution of transformed samples with the minority distribution, we minimize the MMD. For a characteristic kernel $k(\cdot, \cdot)$ on an RKHS $\mathcal{H}_k$ with feature map $\phi$, the squared MMD between $P$ and $Q$ is
\begin{equation}\label{equation:MMD_norm}
\mathrm{MMD}^2(P, Q) = \bigl\|\mathbb{E}_{\mathbf{x} \sim P}[\phi(\mathbf{x})] - \mathbb{E}_{\mathbf{z} \sim Q}[\phi(\mathbf{z})]\bigr\|_{\mathcal{H}}^2.
\end{equation}
By the kernel trick,
\begin{equation}
\mathrm{MMD}^2(P,Q) = \mathbb{E}_{\mathbf{x}, \mathbf{x}' \sim P}[k(\mathbf{x}, \mathbf{x}')] + \mathbb{E}_{\mathbf{z}, \mathbf{z}' \sim Q}[k(\mathbf{z}, \mathbf{z}')] - 2\,\mathbb{E}_{\mathbf{x} \sim P, \mathbf{z} \sim Q}[k(\mathbf{x}, \mathbf{z})],
\end{equation}
with independent copies $\mathbf{x}', \mathbf{z}'$. A key property from \citet{gretton2012kernel} is: if $k$ is characteristic, then $\mathrm{MMD}(P,Q)=0$ iff $P=Q$. This guarantees that minimizing MMD between $f_\theta(\mathcal{D}_{\text{maj}})$ and $\mathcal{D}_{\text{min}}$ drives distributional alignment.

Given finite sets $\mathcal{D}_{\text{maj}}$ and $\mathcal{D}_{\text{min}}$, we use the biased empirical estimator:
\begin{align}
\mathcal{L}_{\text{MMD}}
&= \frac{1}{n_{\text{maj}}^{2}}
   \sum_{i,j=1}^{n_{\text{maj}}}
   k\!\left(f_{\theta}(\mathbf{x}_{i}),\, f_{\theta}(\mathbf{x}_{j})\right) \notag\\
&\quad + \frac{1}{n_{\text{min}}^{2}}
   \sum_{i,j=1}^{n_{\text{min}}}
   k(\mathbf{z}_{i},\, \mathbf{z}_{j})
   - \frac{2}{n_{\text{maj}} n_{\text{min}}}
   \sum_{i=1}^{n_{\text{maj}}}\sum_{j=1}^{n_{\text{min}}}
   k\!\bigl(f_{\theta}(\mathbf{x}_{i}),\, \mathbf{z}_{j}\bigr).
\end{align}
where we use a Gaussian kernel $k(\mathbf{u}, \mathbf{v}) = \exp\!\left(-\|\mathbf{u} - \mathbf{v}\|_2^2/2\sigma^2 \right)$ with bandwidth $\sigma$ by the median heuristic.

\subsection{Triplet Loss Regularizer}
% While MMD ensures global alignment, it does not guarantee that generated samples lie near the decision boundary. We therefore employ a triplet loss \citep{schroff2015facenet}. Anchors are transformed majority samples $\tilde{\mathbf{x}} = f_\theta(\mathbf{x})$ with $\mathbf{x} \in \mathcal{D}_{\mathrm{maj}}$. Inspired by Borderline-SMOTE \citep{han2005borderline}, positives $\mathbf{x}^+$ are minority samples in a \emph{danger set} (having more than $k/2$ but fewer than $k$ majority neighbors among their $k$ nearest neighbors), while negatives $\mathbf{x}^-$ are majority instances from a \emph{safe set}. The loss enforces
% \begin{equation*}
% \mathcal{L}_{\text{triplet}} = \max\!\left(\|\tilde{\mathbf{x}} - \mathbf{x}^+\|_2^2 - \|\tilde{\mathbf{x}} - \mathbf{x}^-\|_2^2 + \alpha,\, 0\right),
% \end{equation*}
% encouraging boundary-proximal synthetic samples while maintaining separation from majority interiors.
While MMD ensures global alignment, it does not guarantee that generated samples lie near the decision boundary. To address this, we employ a triplet loss regularizer \citep{schroff2015facenet, weinberger2009distance} that explicitly encourages transformed samples to populate informative boundary regions. The triplet loss operates on three types of samples: (i) \emph{anchors}, which are transformed majority samples $\tilde{\mathbf{x}}_i = f_\theta(\mathbf{x}_i^{\text{maj}})$ for $\mathbf{x}_i^{\text{maj}} \in \mathcal{D}_{\text{maj}}$; (ii) \emph{positives}, which are minority samples from a danger set near the class boundary; and (iii) \emph{negatives}, which are majority samples from a safe set far from the boundary. We now formally define these sets and describe the sampling strategy for reproducibility.

\vspace{0.5em}
\noindent\textbf{Definition 1 (Danger Set).} For a minority sample $\mathbf{x}_j^{\text{min}} \in \mathcal{D}_{\text{min}}$, let $\mathcal{N}_k(\mathbf{x}_j^{\text{min}})$ denote its $k$ nearest neighbors in $\mathcal{D}_{\text{all}} = \mathcal{D}_{\text{min}} \cup \mathcal{D}_{\text{maj}}$ under Euclidean distance. Define $c_{\text{maj}}(\mathbf{x}_j^{\text{min}}) = |\mathcal{N}_k(\mathbf{x}_j^{\text{min}}) \cap \mathcal{D}_{\text{maj}}|$ as the count of majority-class neighbors. The \emph{danger set} is
\begin{equation*}
\mathcal{D}_{\text{danger}} = \left\{\mathbf{x}_j^{\text{min}} \in \mathcal{D}_{\text{min}} \,:\, \frac{k}{2} < c_{\text{maj}}(\mathbf{x}_j^{\text{min}}) < k \right\}.
\end{equation*}
Intuitively, danger samples are minority instances surrounded by a significant number of majority neighbors, placing them near the class boundary \citep{han2005borderline}.

\vspace{0.5em}
\noindent\textbf{Definition 2 (Safe Majority Set).} For a majority sample $\mathbf{x}_i^{\text{maj}} \in \mathcal{D}_{\text{maj}}$, define its distance to the minority class as $d_{\text{min}}(\mathbf{x}_i^{\text{maj}}) = \min_{\mathbf{x}^{\text{min}} \in \mathcal{D}_{\text{min}}} \|\mathbf{x}_i^{\text{maj}} - \mathbf{x}^{\text{min}}\|_2$. Let $\tau_q$ denote the $q$-th quantile of $\{d_{\text{min}}(\mathbf{x}_i^{\text{maj}}) : \mathbf{x}_i^{\text{maj}} \in \mathcal{D}_{\text{maj}}\}$ for $q \in (0,1)$. The \emph{safe majority set} is
\begin{equation*}
\mathcal{D}_{\text{safe}} = \left\{\mathbf{x}_i^{\text{maj}} \in \mathcal{D}_{\text{maj}} \,:\, d_{\text{min}}(\mathbf{x}_i^{\text{maj}}) > \tau_q \right\}.
\end{equation*}
Safe majority samples are those far from the decision boundary, representing the interior of the majority distribution.

\vspace{0.5em}
\noindent Given these definitions, the triplet loss for a transformed sample $\tilde{\mathbf{x}}_i = f_\theta(\mathbf{x}_i^{\text{maj}})$ is computed by finding its nearest positive (from $\mathcal{D}_{\text{danger}}$) and nearest negative (from $\mathcal{D}_{\text{safe}}$):
\begin{equation}\label{eq:triplet_loss}
\mathcal{L}_{\text{triplet}} = \frac{1}{n_{\text{maj}}} \sum_{i=1}^{n_{\text{maj}}} \max\left(d_{\text{pos}}(\tilde{\mathbf{x}}_i) - d_{\text{neg}}(\tilde{\mathbf{x}}_i) + \alpha,\, 0\right),
\end{equation}
where 
\begin{align*}
d_{\text{pos}}(\tilde{\mathbf{x}}_i) &= \min_{\mathbf{x}^{\text{min}} \in \mathcal{D}_{\text{danger}}} \|\tilde{\mathbf{x}}_i - \mathbf{x}^{\text{min}}\|_2^2, \\
d_{\text{neg}}(\tilde{\mathbf{x}}_i) &= \min_{\mathbf{x}^{\text{maj}} \in \mathcal{D}_{\text{safe}}} \|\tilde{\mathbf{x}}_i - \mathbf{x}^{\text{maj}}\|_2^2,
\end{align*}
and $\alpha > 0$ is a fixed margin parameter that controls the desired separation between positive and negative samples. 

Algorithm~\ref{alg:triplet_sampling} provides a complete specification of the triplet set construction and loss computation procedure, ensuring full reproducibility.

\begin{algorithm}[ht!]
\caption{Triplet Set Construction and Loss Computation}
\label{alg:triplet_sampling}
{\small
\begin{algorithmic}[1]
\Require $\mathcal{D}_{\text{min}}$, $\mathcal{D}_{\text{maj}}$, transformation $f_\theta$, hyperparameters $k{=}5$, $q{=}0.75$, margin $\alpha{=}1.0$
\Ensure Triplet loss $\mathcal{L}_{\text{triplet}}$
\State \textbf{// Phase 1: Construct Danger Set} (computed once before training)
\State $\mathcal{D}_{\text{all}} \gets \mathcal{D}_{\text{min}} \cup \mathcal{D}_{\text{maj}}$, \quad $\mathcal{D}_{\text{danger}} \gets \emptyset$
\For{$\mathbf{x}_j^{\text{min}} \in \mathcal{D}_{\text{min}}$}
    \State Find $k$ nearest neighbors $\mathcal{N}_k(\mathbf{x}_j^{\text{min}})$ in $\mathcal{D}_{\text{all}}$
    \State $c_{\text{maj}} \gets |\mathcal{N}_k(\mathbf{x}_j^{\text{min}}) \cap \mathcal{D}_{\text{maj}}|$
    \If{$k/2 < c_{\text{maj}} < k$} 
        \State $\mathcal{D}_{\text{danger}} \gets \mathcal{D}_{\text{danger}} \cup \{\mathbf{x}_j^{\text{min}}\}$
    \EndIf
\EndFor
\State \textbf{// Phase 2: Construct Safe Majority Set} (computed once before training)
\For{$\mathbf{x}_i^{\text{maj}} \in \mathcal{D}_{\text{maj}}$}
    \State $d_{\text{min}}(\mathbf{x}_i^{\text{maj}}) \gets \min_{\mathbf{x}^{\text{min}} \in \mathcal{D}_{\text{min}}} \|\mathbf{x}_i^{\text{maj}} - \mathbf{x}^{\text{min}}\|_2$
\EndFor
\State $\tau_q \gets$ $q$-th quantile of $\{d_{\text{min}}(\mathbf{x}_i^{\text{maj}})\}$
\State $\mathcal{D}_{\text{safe}} \gets \{\mathbf{x}_i^{\text{maj}} : d_{\text{min}}(\mathbf{x}_i^{\text{maj}}) > \tau_q\}$
\State \textbf{// Phase 3: Compute Triplet Loss} (executed each training iteration)
\State $\mathcal{L}_{\text{triplet}} \gets 0$
\For{$i = 1$ \textbf{to} $n_{\text{maj}}$}
    \State $\tilde{\mathbf{x}}_i \gets f_\theta(\mathbf{x}_i^{\text{maj}})$
    \State $d_{\text{pos}} \gets \min_{\mathbf{x}^{\text{min}} \in \mathcal{D}_{\text{danger}}} \|\tilde{\mathbf{x}}_i - \mathbf{x}^{\text{min}}\|_2^2$
    \State $d_{\text{neg}} \gets \min_{\mathbf{x}^{\text{maj}} \in \mathcal{D}_{\text{safe}}} \|\tilde{\mathbf{x}}_i - \mathbf{x}^{\text{maj}}\|_2^2$
    \State $\mathcal{L}_{\text{triplet}} \gets \mathcal{L}_{\text{triplet}} + \max(d_{\text{pos}} - d_{\text{neg}} + \alpha, 0)$
\EndFor
\State \Return $\mathcal{L}_{\text{triplet}} / n_{\text{maj}}$
\end{algorithmic}
}
\end{algorithm}

\vspace{0.5em}
\noindent\textbf{Implementation Details.} We fix $k{=}5$ across all datasets, following the Borderline-SMOTE convention \citep{han2005borderline}. The quantile threshold $q{=}0.75$ ensures that $\mathcal{D}_{\text{safe}}$ comprises samples in the top quartile of distance from the minority class. Crucially, both $\mathcal{D}_{\text{danger}}$ and $\mathcal{D}_{\text{safe}}$ are computed once before training (Phases 1--2), as they depend only on the fixed original data distributions. This design offers computational efficiency: nearest-neighbor queries and distance computations are performed once rather than at every training iteration. During training (Phase 3), only distances from transformed samples $\tilde{\mathbf{x}}_i = f_\theta(\mathbf{x}_i^{\text{maj}})$ to these precomputed sets are evaluated. The margin parameter is fixed at $\alpha{=}1.0$ across all datasets.




\section{Simulation Studies}
\label{sec:simulation_studies}

In this section, we provide a qualitative assessment of our proposed oversampling framework through visual analysis on two synthetic datasets. We consider two scenarios: (i) a Gaussian Blob dataset and (ii) a Half-moon dataset.

\paragraph{Gaussian Blob} This simulation evaluates a method's capacity to generate meaningful samples in a scenario with overlapping class distributions. The majority and minority classes, $D_{\text{maj}}$ and $D_{\text{min}}$, are drawn from two-dimensional Gaussian distributions. Specifically, the majority class follows $N(\mu_0, 0.5I_2)$ with $\mu_0 = (0, 0)$, while the minority class follows $N(\mu_1, I_2)$ with $\mu_1 = (0.5, 0.5)$, where $I_2$ is a two-dimensional identity matrix. This setup creates a region of class overlap, which presents a challenge for oversampling methods that must generate samples that are not only distributionally aligned but also located in informative, boundary-proximal regions. The objective is to evaluate whether the learned transformation can effectively generate minority-class samples while preserving the data’s intrinsic geometric structure. As shown in Figure~\ref{fig:gaussian_blob}, our method successfully transforms majority samples into the minority cluster, generating synthetic instances that are well-aligned with the target distribution. The MMD loss ensures that the generated samples cover the full spread of the minority class, while the triplet loss regularizer steers them toward the decision boundary.

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0\linewidth]{Figures/gaussian_blob_synthetic_experiment.pdf}
\caption{Visualization of oversampling on the Gaussian Blob dataset. The majority (blue) and minority (orange) classes exhibit overlap. The samples generated by each method are represented in green. Our method generates samples that align well with the target minority distribution while effectively populating the decision boundary.}
\label{fig:gaussian_blob}
\end{figure}

\paragraph{Half-moon} The Half-moon dataset is employed to assess the framework's ability to learn a highly non-linear transformation. The data consists of two interleaving half-moon shapes, with a noise level of 0.25 introduced to create ambiguity along the class boundary. This configuration tests the model's flexibility in mapping a geometrically simple distribution onto a complex, non-convex manifold. As depicted in Figure~\ref{fig:half_moon}, our method demonstrates a strong capability to learn the required non-linear mapping. The generated samples conform closely to the crescent shape of the minority class, indicating that the transformation successfully captures the underlying data manifold. The integration of MMD ensures that the density of the generated samples matches that of the original minority moon, while the triplet loss places these samples in regions most beneficial for defining the non-linear decision boundary. Other generative models often fail to capture complex distributions under data sparsity, yielding incoherent samples; this highlights the advantage of our transport-map–based approach, which uses abundant majority-class information to guide the generation process.

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0\linewidth]{Figures/halfmoon_synthetic_experiment.pdf}
\caption{Visualization of oversampling on the Half-moon dataset, a standard benchmark for evaluating performance on non-linear manifolds. The samples generated by each method are represented in green. Our method successfully learns the required non-linear transformation, generating synthetic samples that conform to the geometric structure of the minority class.}
\label{fig:half_moon}
\end{figure}


\section{Real data Experiments}
\label{sec:real_data_exp}
In this section, we evaluate our proposed oversampling framework on real-world imbalanced datasets to demonstrate its effectiveness in enhancing classification performance under varying degrees of class imbalance. We assess the effectiveness of our method in comparison to established baselines across various performance metrics. 

\subsection{Experimental Setup}\label{subsec:setup}
We evaluate the proposed method on 29 real-world imbalanced datasets commonly used in the literature \citep{chawla2002smote, barua2012mwmote}. Table~\ref{tab:dataset_summary} summarizes key characteristics of these datasets from diverse domains, including total sample size ($N_{\text{total}}$), minority sample size ($N_{\text{min}}$), imbalance ratio (IR), and feature dimension ($d$). Their wide range of IR values provides a robust testbed for evaluating oversampling methods across varying degrees of class imbalance.

\setlength{\LTcapwidth}{\textwidth}
\begin{longtable}{@{\extracolsep{\fill}} c c c c c c }
\caption{Summary of 29 real-world imbalanced datasets employed in experiments. Each dataset is assigned a shorthand identifier ($D_1$--$D_{29}$) for clarity in subsequent result tables. Data characteristics include total size ($N_{\mathrm{total}}$), minority size ($N_{\mathrm{min}}$), imbalance ratio (IR), and feature dimension ($d$).}
\label{tab:dataset_summary}\\
\toprule
ID & Dataset & $N_{\text{total}}$ & $N_{\text{min}}$ & IR & $d$ \\
\midrule
\endfirsthead
\multicolumn{6}{l}{\textit{Continued from previous page}} \\
\toprule
ID & Dataset & $N_{\text{total}}$ & $N_{\text{min}}$ & IR & $d$ \\
\midrule
\endhead
\bottomrule
\multicolumn{6}{r}{\textit{Continued on next page}} \\
\endfoot
\bottomrule
\endlastfoot

$D_{1}$ & \texttt{abalone19} & 4174 & 31 & 129.44 & 8 \\
$D_{2}$ & \texttt{abalone9-18} & 731 & 42 & 16.4 & 8 \\
$D_{3}$ & \texttt{arrhythmia} & 452 & 25 & 17 & 278 \\
$D_{4}$ & \texttt{cleveland-0\_vs\_4} & 177 & 12 & 12.62 & 13 \\
$D_{5}$ & \texttt{coil\_2000} & 9822 & 577 & 16 & 85 \\
$D_{6}$ & \texttt{ecoli3} & 336 & 35 & 8.6 & 7 \\
$D_{7}$ & \texttt{ecoli4} & 336 & 20 & 15.8 & 7 \\
$D_{8}$ & \texttt{glass-0-1-2-3\_vs\_4-5-6} & 214 & 50 & 3.2 & 9 \\
$D_{9}$ & \texttt{glass-0-1-6\_vs\_5} & 184 & 9 & 19.44 & 9 \\
$D_{10}$ & \texttt{glass4} & 214 & 12 & 15.47 & 9 \\
$D_{11}$ & \texttt{oil} & 937 & 40 & 22 & 49 \\
$D_{12}$ & \texttt{ozone\_level} & 2536 & 72 & 34 & 72 \\
$D_{13}$ & \texttt{pima} & 768 & 267 & 1.87 & 8 \\
$D_{14}$ & \texttt{scene} & 2407 & 171 & 13 & 294 \\
$D_{15}$ & \texttt{segment0} & 2308 & 328 & 62 & 19 \\
$D_{16}$ & \texttt{sick\_euthyroid} & 3163 & 292 & 9.8 & 42 \\
$D_{17}$ & \texttt{solar\_flare\_m0} & 1389 & 69 & 19 & 32 \\
$D_{18}$ & \texttt{thyroid\_sick} & 3772 & 235 & 15 & 52 \\
$D_{19}$ & \texttt{us\_crime} & 1994 & 153 & 12 & 100 \\
$D_{20}$ & \texttt{wine\_quality} & 4898 & 181 & 26 & 11 \\
$D_{21}$ & \texttt{winequality-red-3\_vs\_5} & 691 & 10 & 68.1 & 11 \\
$D_{22}$ & \texttt{winequality-red-4} & 1599 & 52 & 29.17 & 11 \\
$D_{23}$ & \texttt{winequality-white-3\_vs\_7} & 900 & 20 & 44 & 11 \\
$D_{24}$ & \texttt{winequality-white-3-9\_vs\_5} & 1482 & 25 & 58.28 & 11 \\
$D_{25}$ & \texttt{yeast-0-5-6-7-9\_vs\_4} & 528 & 51 & 9.35 & 8 \\
$D_{26}$ & \texttt{yeast-1-2-8-9\_vs\_7} & 947 & 29 & 30.57 & 8 \\
$D_{27}$ & \texttt{yeast-1-4-5-8\_vs\_7} & 693 & 30 & 22.1 & 8 \\
$D_{28}$ & \texttt{yeast4} & 1484 & 50 & 28.1 & 8 \\
$D_{29}$ & \texttt{yeast6} & 1484 & 35 & 41.4 & 8 \\
\end{longtable}



We benchmark our method against a comprehensive set of state-of-the-art oversampling algorithms, each representing a distinct approach to imbalanced classification. For each baseline, minority class oversampling is applied solely to the training data until class balance is reached, prior to classifier training. Unless otherwise specified, a Support Vector Machine (SVM) with a radial basis function (RBF) kernel serves as the base learner, consistent with established protocols in imbalanced learning research \citep{krawczyk2016learning}. Default hyperparameters are adopted for all methods, following recommendations in their respective original works. The compared methods include: 

\begin{itemize}
    \item \textbf{Random OverSampling (ROS)} \citep{he2009learning}: Uniformly duplicates minority instances until class balance is achieved, introducing no synthetic variation but serving as a canonical baseline for assessing the impact of sample size alone. 
    \item \textbf{SMOTE} \citep{chawla2002smote}: Produces synthetic minority examples by linearly interpolating between a minority instance and its k-nearest minority neighbors, thereby increasing within-class diversity and expanding the minority support. 
    \item \textbf{Borderline-SMOTE (bSMOTE)} \citep{han2005borderline}: Focuses synthetic generation on so-called danger samples near the decision boundary, as determined by local neighborhood structure, to augment boundary informativeness. 
    \item \textbf{ADASYN} \citep{he2008adasyn}: Allocates more synthetic generation to minority samples surrounded by a larger fraction of majority class neighbors, adaptively addressing local imbalance in the feature space.
    \item \textbf{MWMOTE} \citep{barua2012mwmote}: Employs an instance-weighting scheme to prioritize minority samples in regions deemed most informative for classification, based on proximity to the majority class and cluster structure. 
    \item \textbf{CTGAN} \citep{xu2019modeling}: uses a conditional generative adversarial network specifically designed for structured tabular data, enabling high-fidelity synthetic minority sample generation by modeling conditional distribution. 
    \item \textbf{GAMO} \citep{mullick2019generative}: Augments adversarial learning with an auxiliary classifier to encourage generation of synthetic minority samples that are decision-boundary-aware. 
    \item \textbf{MGVAE} \citep{ai2023generative}: Leverages a majority-guided variational autoencoder to transfer diversity from the majority to the minority class, synthesizing realistic minority data through a learned prior informed by majority characteristics. 
\end{itemize}

\vspace{1em}


\noindent Performance is evaluated using four widely adopted metrics that are considered suitable for class imbalanced learning, each characterizing different facets of performance with sensitivity to minority class behavior. The metrics are defined and interpreted as follows: 
\begin{itemize}
    \item \textbf{Area Under Receiver Operating Characteristic Curve (AUROC)} \citep{fawcett2006introduction}: Measures the probability that a randomly selected positive instance ranks higher than a randomly selected negative instance according to the classifier's predicted scores. Formally, for a scoring function $s(\cdot)$, AUROC computes
    \begin{equation*}
        \text{AUROC} = \mathbb{P}\left( s(x^+) > s(x^-)\right),
    \end{equation*}
    where $x^+ \sim \mathcal{D}_{\text{min}}$ and $x^- \sim \mathcal{D}_{\text{maj}}$. The metric is threshold-independent and robust to imbalance because it evaluates ranking rather than absolute predictions. 

\item \textbf{Geometric Mean (G-mean)} \citep{he2009learning}:  
Defined as the square root of the product of the true positive rate (TPR) and true negative rate (TNR), this metric penalizes classifiers that favor one class over the other. For binary classification,  
\[
   \mathrm{G\text{-}mean} 
   = \sqrt{\mathrm{TPR} \times \mathrm{TNR}}
   = \sqrt{\frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} \times \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FP}}}\,,
\]
where 
\begin{itemize}
  \item TP: true positives (correctly predicted positive instances),  
  \item FN: false negatives (positive instances incorrectly predicted as negative),  
  \item TN: true negatives (correctly predicted negative instances),  
  \item FP: false positives (negative instances incorrectly predicted as positive).  
\end{itemize}
A high G-mean indicates strong, balanced performance on both classes.

    \item \textbf{F1-score}: Captures the harmonic mean of precision and recall with the formula:
    \begin{equation*}
        \text{F1} = \frac{2\times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2\times\text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}.
    \end{equation*}
It measures the balance between precision and recall for the minority class, placing equal emphasis on both false negatives and false positives. 

    \item \textbf{Matthews Correlation Coefficient (MCC)} \citep{chicco2020advantages}: Provides a correlation coefficient between the predicted and true labels, defined as:
    \begin{equation*}
        \text{MCC} = \frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP}+ \text{FP}) \times (\text{TP} + \text{FN}) \times (\text{TN} + \text{FP}) \times (\text{TN} + \text{FN})}}.
    \end{equation*}
    $\text{MCC}$ ranges from $-1$ to $1$, where $1$ indicates perfect prediction, $0$ corresponds to random guessing, and $-1$ reflects complete disagreement.
\end{itemize}

\vspace{1em}
\noindent Evaluation is conducted through 10-fold cross-validation repeated over 10 independent trials, following the work of \citet{mostafaei2023ouboost}. All results are reported as mean values across folds, along with per-dataset ranks to facilitate comparative analysis. 

\subsection{Main Results}\label{subsec: main_results}

We present comparative results across 29 real-world benchmark datasets, evaluating the effectiveness of our proposed oversampling framework against both traditional and generative baselines. Performance is measured in terms of AUROC, G-mean, F1-score, and MCC as defined in Section~\ref{subsec:setup}. To ensure robust conclusions, we report average scores with their corresponding ranks and assess statistical significance via paired t-tests comparing our proposed method both to the Original (non-oversampled) baseline and to the best-performing alternative method. 

\noindent \textcolor{blue}{The transformation network $f_\theta : \mathbb{R}^d \to \mathbb{R}^d$ consists of three hidden layers with dimensions $[2d, 4d, 2d]$ and residual structure $f_\theta(\mathbf{x}) = \mathbf{x} + h_\theta(\mathbf{x})$, where each layer applies BatchNorm--ReLU--Dropout($0.2$). We train with Adam (lr=$10^{-3}$, max 500 epochs, early stopping patience 50) and fix $\lambda{=}0.01$, $k{=}5$, $\alpha{=}1.0$, $\sigma$ via median heuristic. This architecture remains consistent across all datasets. Full implementation details are available at \url{https://github.com/suman-cha/moms-imbalanced-learning}.} 


\vspace{0.5em}
\noindent \textbf{Main Performance.} Tables~\ref{tab:auroc_results}–\ref{tab:mcc_results} summarize the average performance metrics of all methods, with ranks indicating their relative ordering for each dataset. Our method achieves the highest average rank among all baselines, demonstrating consistent superiority across datasets with diverse properties, including extreme imbalance ratios and high-dimensional feature spaces.

\begin{table}[ht!]
\centering
\caption{Comparison of AUROC performance across 29 imbalanced datasets for baseline (Orig) and oversampling methods: ROS, SMOTE (SM), Borderline-SMOTE (bSM), ADASYN (ADA), MWMOTE (MW), CTGAN (GAN), GAMO (GAM), MGVAE (MV), MMD, and MMD+Triplet (MMD+T). Bold and underlined values indicate best and second-best performance per dataset. R1(2): number of first(second)-place rankings; Avg: average rank across all datasets.}
\label{tab:auroc_results}
\vspace{5pt}
{\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c c c c c c c c c c c c}
\toprule
Data & Orig & ROS & SM & bSM & ADA & MW & GAN & GAM & MV & MMD & MMD+T \\
\midrule
 $D_{1}$ & 0.750 & 0.787 & 0.790 & 0.784 & 0.789 & 0.738 & 0.565 & 0.561 & 0.744 & 0.808 & \textbf{0.808} \\
 $D_{2}$ & 0.840 & \textbf{0.904} & 0.894 & 0.877 & 0.883 & 0.878 & 0.844 & 0.838 & 0.853 & \underline{0.900} & 0.900 \\
 $D_{3}$ & 0.880 & \textbf{0.919} & 0.911 & 0.898 & 0.912 & 0.904 & 0.794 & 0.701 & 0.810 & \underline{0.913} & 0.900 \\
 $D_{4}$ & \textbf{0.986} & 0.983 & 0.984 & 0.983 & 0.983 & 0.983 & 0.977 & 0.897 & 0.984 & \underline{0.986} & 0.986 \\
 $D_{5}$ & 0.648 & \textbf{0.703} & 0.680 & 0.680 & 0.681 & 0.668 & 0.618 & 0.597 & 0.645 & 0.689 & \underline{0.700} \\
 $D_{6}$ & 0.953 & \underline{0.954} & \textbf{0.956} & 0.948 & 0.951 & 0.950 & 0.949 & 0.902 & 0.951 & 0.954 & 0.954 \\
 $D_{7}$ & 0.961 & 0.963 & 0.968 & 0.924 & 0.959 & 0.959 & 0.972 & 0.962 & 0.962 & \textbf{0.978} & \underline{0.977} \\
 $D_{8}$ & 0.978 & 0.981 & 0.979 & 0.979 & 0.977 & 0.975 & 0.979 & \textbf{0.985} & 0.980 & \underline{0.983} & 0.983 \\
 $D_{9}$ & 0.969 & 0.985 & 0.985 & 0.982 & 0.986 & 0.977 & \textbf{0.989} & 0.980 & 0.978 & \underline{0.987} & 0.987 \\
 $D_{10}$ & 0.989 & 0.986 & \underline{0.991} & 0.989 & 0.990 & 0.989 & 0.961 & 0.981 & \textbf{0.992} & 0.989 & 0.989 \\
 $D_{11}$ & 0.917 & 0.912 & 0.913 & \underline{0.931} & 0.913 & 0.914 & 0.896 & 0.888 & \textbf{0.932} & 0.931 & 0.922 \\
 $D_{12}$ & 0.833 & 0.866 & 0.869 & 0.893 & 0.866 & 0.872 & 0.847 & 0.721 & 0.872 & \underline{0.900} & \textbf{0.906} \\
 $D_{13}$ & 0.829 & 0.828 & 0.825 & 0.818 & 0.822 & 0.818 & 0.824 & 0.812 & 0.827 & \underline{0.830} & \textbf{0.831} \\
 $D_{14}$ & 0.768 & 0.782 & 0.779 & 0.772 & 0.775 & 0.774 & 0.740 & 0.681 & \underline{0.785} & 0.782 & \textbf{0.789} \\
 $D_{15}$ & \textbf{1.000} & \underline{1.000} & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 0.995 & 1.000 & 1.000 & 1.000 \\
 $D_{16}$ & 0.942 & \textbf{0.951} & \underline{0.950} & 0.945 & 0.950 & 0.945 & 0.931 & 0.912 & 0.940 & 0.945 & 0.946 \\
 $D_{17}$ & 0.659 & \underline{0.692} & 0.661 & 0.672 & 0.657 & 0.648 & \textbf{0.693} & 0.616 & 0.666 & 0.681 & 0.684 \\
 $D_{18}$ & 0.941 & \underline{0.955} & 0.955 & 0.947 & \textbf{0.956} & 0.948 & 0.932 & 0.922 & 0.940 & 0.946 & 0.948 \\
 $D_{19}$ & 0.866 & 0.908 & 0.901 & \underline{0.914} & 0.894 & 0.900 & 0.892 & 0.862 & 0.868 & 0.914 & \textbf{0.918} \\
 $D_{20}$ & 0.771 & \textbf{0.821} & 0.804 & 0.805 & 0.802 & 0.781 & 0.774 & 0.693 & 0.752 & 0.799 & \underline{0.815} \\
 $D_{21}$ & 0.674 & 0.735 & 0.635 & 0.673 & 0.631 & 0.683 & \textbf{0.817} & \underline{0.752} & 0.699 & 0.716 & 0.751 \\
 $D_{22}$ & 0.676 & 0.744 & 0.719 & 0.730 & 0.720 & 0.719 & 0.691 & 0.661 & 0.576 & \textbf{0.751} & \underline{0.747} \\
 $D_{23}$ & \underline{0.813} & 0.651 & 0.632 & 0.736 & 0.635 & 0.625 & 0.794 & 0.776 & 0.762 & \textbf{0.884} & 0.707 \\
 $D_{24}$ & 0.880 & 0.849 & 0.868 & \underline{0.892} & 0.878 & 0.842 & 0.847 & \textbf{0.901} & 0.848 & 0.711 & 0.869 \\
 $D_{25}$ & 0.885 & 0.885 & 0.883 & 0.882 & 0.877 & 0.872 & 0.847 & 0.811 & \textbf{0.888} & \underline{0.887} & 0.886 \\
 $D_{26}$ & 0.661 & 0.732 & 0.711 & \textbf{0.751} & 0.707 & 0.702 & 0.737 & 0.625 & 0.613 & 0.739 & \underline{0.747} \\
 $D_{27}$ & 0.639 & 0.678 & 0.678 & \textbf{0.702} & 0.663 & 0.676 & 0.638 & 0.537 & 0.585 & \underline{0.689} & 0.683 \\
 $D_{28}$ & 0.842 & 0.896 & 0.896 & 0.898 & 0.892 & 0.891 & 0.848 & 0.734 & 0.854 & \underline{0.906} & \textbf{0.908} \\
 $D_{29}$ & 0.838 & 0.925 & \underline{0.934} & \textbf{0.937} & 0.924 & 0.934 & 0.929 & 0.768 & 0.906 & 0.932 & 0.933 \\
\midrule
R1(2) & 2 (1) & 5 (4) & 1 (3) & 3 (3) & 1 (0) & 0 (0) & 3 (0) & 2 (1) & 3 (1) & 3 (10) & 6 (5) \\
Avg & 7.34 & 4.47 & 5.09 & 5.48 & 6.22 & 7.31 & 7.50 & 9.40 & 6.78 & \underline{3.41} & \textbf{3.00} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht!]
\caption{Comparison of G-mean performance across 29 imbalanced datasets for baseline (Orig) and oversampling methods: ROS, SMOTE (SM), Borderline-SMOTE (bSM), ADASYN (ADA), MWMOTE (MW), CTGAN (GAN), GAMO (GAM), MGVAE (MV), MMD, and MMD+Triplet (MMD+T). Bold and underlined values indicate best and second-best performance per dataset. R1(2): number of first(second)-place rankings; Avg: average rank across all datasets.}
\centering
\label{tab:gmean_results}
\vspace{5pt}
{\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c c c c c c c c c c c c}
\toprule
Data & Orig & ROS & SM & bSM & ADA & MW & GAN & GAM & MV & MMD & MMD+T \\
\midrule
 $D_{1}$ & 0.000 & 0.620 & 0.616 & 0.307 & 0.620 & 0.296 & 0.000 & 0.000 & 0.000 & \underline{0.671} & \textbf{0.683} \\
 $D_{2}$ & 0.168 & \textbf{0.806} & 0.757 & 0.704 & 0.744 & 0.726 & 0.312 & 0.462 & 0.522 & \underline{0.788} & 0.786 \\
 $D_{3}$ & 0.000 & 0.486 & 0.467 & 0.435 & 0.460 & 0.414 & 0.000 & 0.122 & 0.000 & \underline{0.610} & \textbf{0.766} \\
 $D_{4}$ & 0.262 & 0.753 & 0.667 & 0.625 & 0.667 & 0.730 & 0.768 & 0.685 & 0.260 & \underline{0.801} & \textbf{0.811} \\
 $D_{5}$ & 0.001 & \textbf{0.592} & 0.552 & 0.532 & \underline{0.554} & 0.485 & 0.030 & 0.154 & 0.003 & 0.395 & 0.541 \\
 $D_{6}$ & 0.729 & 0.880 & 0.872 & 0.869 & 0.866 & 0.869 & \underline{0.882} & 0.796 & 0.766 & 0.881 & \textbf{0.884} \\
 $D_{7}$ & 0.827 & 0.841 & 0.850 & 0.830 & \textbf{0.871} & 0.864 & \underline{0.870} & 0.870 & 0.854 & 0.860 & 0.861 \\
 $D_{8}$ & 0.913 & 0.934 & 0.939 & 0.933 & 0.941 & 0.938 & 0.916 & \textbf{0.955} & 0.916 & \underline{0.952} & 0.950 \\
 $D_{9}$ & 0.270 & 0.623 & 0.605 & 0.605 & 0.615 & 0.595 & \underline{0.728} & \textbf{0.791} & 0.528 & 0.673 & 0.654 \\
 $D_{10}$ & 0.667 & 0.893 & 0.877 & 0.880 & \underline{0.911} & 0.870 & 0.657 & 0.875 & 0.815 & 0.900 & \textbf{0.915} \\
 $D_{11}$ & 0.517 & 0.753 & 0.719 & 0.712 & 0.732 & 0.714 & 0.502 & 0.630 & 0.484 & \textbf{0.770} & \underline{0.766} \\
 $D_{12}$ & 0.000 & 0.737 & 0.722 & 0.680 & 0.715 & 0.695 & 0.000 & 0.083 & 0.168 & \underline{0.784} & \textbf{0.812} \\
 $D_{13}$ & 0.693 & 0.736 & 0.737 & \textbf{0.744} & \underline{0.744} & 0.738 & 0.714 & 0.731 & 0.700 & 0.739 & 0.744 \\
 $D_{14}$ & 0.048 & 0.467 & 0.449 & 0.441 & 0.450 & 0.453 & 0.043 & 0.207 & 0.168 & \underline{0.655} & \textbf{0.693} \\
 $D_{15}$ & 0.989 & \textbf{0.992} & 0.991 & \underline{0.992} & 0.992 & 0.991 & 0.989 & 0.983 & 0.984 & 0.991 & 0.991 \\
 $D_{16}$ & 0.356 & \textbf{0.898} & \underline{0.897} & 0.880 & 0.895 & 0.881 & 0.779 & 0.337 & 0.537 & 0.890 & 0.891 \\
 $D_{17}$ & 0.000 & \textbf{0.592} & 0.534 & 0.533 & 0.538 & 0.427 & 0.266 & 0.142 & 0.000 & \underline{0.567} & 0.561 \\
 $D_{18}$ & 0.219 & 0.888 & \underline{0.891} & 0.871 & \textbf{0.894} & 0.870 & 0.632 & 0.134 & 0.501 & 0.844 & 0.859 \\
 $D_{19}$ & 0.495 & \underline{0.821} & 0.776 & 0.774 & 0.769 & 0.768 & 0.531 & 0.569 & 0.517 & 0.803 & \textbf{0.844} \\
 $D_{20}$ & 0.082 & \textbf{0.720} & \underline{0.712} & 0.679 & 0.712 & 0.674 & 0.376 & 0.478 & 0.148 & 0.662 & 0.684 \\
 $D_{21}$ & 0.000 & \underline{0.198} & 0.194 & 0.137 & 0.155 & 0.178 & 0.149 & \textbf{0.423} & 0.010 & 0.176 & 0.186 \\
 $D_{22}$ & 0.000 & 0.617 & 0.615 & 0.531 & 0.619 & 0.571 & 0.115 & 0.179 & 0.004 & \textbf{0.640} & \underline{0.634} \\
 $D_{23}$ & 0.000 & 0.358 & 0.276 & 0.375 & 0.249 & 0.216 & 0.325 & 0.404 & 0.000 & \textbf{0.533} & \underline{0.421} \\
 $D_{24}$ & 0.000 & 0.423 & 0.283 & 0.394 & 0.268 & 0.119 & \underline{0.575} & \textbf{0.790} & 0.081 & 0.456 & 0.473 \\
 $D_{25}$ & 0.356 & 0.762 & 0.743 & 0.717 & \textbf{0.765} & 0.719 & 0.700 & 0.722 & 0.583 & \underline{0.764} & 0.761 \\
 $D_{26}$ & 0.000 & 0.614 & 0.563 & 0.540 & 0.577 & 0.564 & 0.529 & 0.487 & 0.045 & \underline{0.635} & \textbf{0.653} \\
 $D_{27}$ & 0.000 & 0.494 & 0.529 & 0.502 & 0.523 & 0.466 & 0.296 & 0.174 & 0.108 & \textbf{0.578} & \underline{0.539} \\
 $D_{28}$ & 0.004 & 0.769 & 0.760 & 0.728 & 0.758 & 0.735 & 0.652 & 0.512 & 0.465 & \textbf{0.800} & \underline{0.792} \\
 $D_{29}$ & 0.370 & 0.816 & 0.812 & 0.770 & 0.796 & 0.816 & \textbf{0.850} & 0.635 & 0.727 & 0.845 & \underline{0.846} \\
\midrule
R 1(2) & 0 (0) & 6 (2) & 0 (3) & 1 (1) & 3 (3) & 0 (0) & 1 (4) & 4 (0) & 0 (0) & 5 (10) & 9 (6) \\
Avg & 10.53 & 3.55 & 4.93 & 6.31 & 4.33 & 6.22 & 7.53 & 7.17 & 9.66 & \underline{3.16} & \textbf{2.60} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht!]
\centering
\caption{Comparison of F1-score performance across 29 imbalanced datasets for baseline (Orig) and oversampling methods: ROS, SMOTE (SM), Borderline-SMOTE (bSM), ADASYN (ADA), MWMOTE (MW), CTGAN (GAN), GAMO (GAM), MGVAE (MV), MMD, and MMD+Triplet (MMD+T). Bold and underlined values indicate best and second-best performance per dataset. R1(2): number of first(second)-place rankings; Avg: average rank across all datasets.}
\label{tab:f1_results}
\vspace{5pt}
{\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c c c c c c c c c c c c}
\toprule
Data & Orig & ROS & SM & bSM & ADA & MW & GAN & GAM & MV & MMD & MMD+T \\
\midrule
 $D_{1}$ & 0.000 & 0.047 & \textbf{0.055} & 0.050 & \underline{0.055} & 0.042 & 0.000 & 0.000 & 0.000 & 0.050 & 0.052 \\
 $D_{2}$ & 0.134 & \textbf{0.427} & 0.409 & \underline{0.422} & 0.376 & 0.399 & 0.247 & 0.332 & 0.384 & 0.400 & 0.404 \\
 $D_{3}$ & 0.000 & 0.396 & 0.382 & 0.353 & 0.376 & 0.336 & 0.000 & 0.037 & 0.000 & \underline{0.450} & \textbf{0.454} \\
 $D_{4}$ & 0.247 & 0.699 & 0.627 & 0.588 & 0.627 & 0.680 & 0.654 & 0.377 & 0.247 & \underline{0.707} & \textbf{0.740} \\
 $D_{5}$ & 0.000 & \underline{0.191} & 0.183 & 0.189 & 0.182 & 0.183 & 0.008 & 0.049 & 0.001 & 0.172 & \textbf{0.208} \\
 $D_{6}$ & 0.615 & 0.625 & 0.636 & 0.626 & 0.607 & 0.623 & \textbf{0.649} & 0.522 & \underline{0.649} & 0.613 & 0.618 \\
 $D_{7}$ & \underline{0.809} & 0.639 & 0.716 & 0.608 & 0.636 & 0.634 & 0.695 & 0.541 & \textbf{0.826} & 0.686 & 0.685 \\
 $D_{8}$ & 0.873 & 0.891 & 0.897 & 0.891 & 0.897 & 0.892 & 0.877 & 0.898 & 0.875 & \textbf{0.909} & \underline{0.909} \\
 $D_{9}$ & 0.267 & 0.558 & 0.557 & 0.557 & 0.560 & 0.547 & \textbf{0.625} & 0.579 & 0.507 & \underline{0.605} & 0.592 \\
 $D_{10}$ & 0.638 & \textbf{0.815} & 0.767 & 0.801 & 0.797 & 0.790 & 0.545 & 0.519 & 0.783 & 0.751 & \underline{0.807} \\
 $D_{11}$ & 0.454 & 0.448 & 0.447 & \textbf{0.521} & 0.448 & 0.481 & 0.421 & 0.431 & 0.412 & 0.483 & \underline{0.507} \\
 $D_{12}$ & 0.000 & \underline{0.318} & 0.305 & \textbf{0.321} & 0.299 & 0.302 & 0.000 & 0.024 & 0.110 & 0.310 & 0.301 \\
 $D_{13}$ & 0.616 & 0.663 & 0.663 & \textbf{0.675} & \underline{0.674} & 0.665 & 0.636 & 0.657 & 0.623 & 0.666 & 0.672 \\
 $D_{14}$ & 0.022 & \textbf{0.274} & 0.253 & 0.267 & 0.254 & 0.251 & 0.019 & 0.085 & 0.086 & 0.267 & \underline{0.272} \\
 $D_{15}$ & 0.987 & \textbf{0.989} & \underline{0.989} & 0.970 & 0.971 & 0.989 & 0.978 & 0.959 & 0.981 & 0.988 & 0.989 \\
 $D_{16}$ & 0.223 & 0.610 & \underline{0.627} & 0.605 & 0.599 & 0.620 & \textbf{0.649} & 0.194 & 0.426 & 0.608 & 0.606 \\
 $D_{17}$ & 0.000 & 0.179 & 0.157 & 0.163 & 0.158 & 0.142 & 0.149 & 0.076 & 0.000 & \textbf{0.206} & \underline{0.187} \\
 $D_{18}$ & 0.111 & 0.541 & \textbf{0.586} & \underline{0.575} & 0.566 & 0.575 & 0.510 & 0.059 & 0.384 & 0.560 & 0.543 \\
 $D_{19}$ & 0.376 & \textbf{0.519} & 0.486 & \underline{0.513} & 0.475 & 0.484 & 0.412 & 0.434 & 0.400 & 0.505 & 0.513 \\
 $D_{20}$ & 0.037 & 0.248 & 0.246 & \textbf{0.265} & 0.241 & 0.221 & 0.201 & 0.160 & 0.071 & 0.240 & \underline{0.251} \\
 $D_{21}$ & 0.000 & \underline{0.121} & 0.071 & 0.056 & 0.052 & 0.116 & 0.118 & \textbf{0.137} & 0.010 & 0.080 & 0.084 \\
 $D_{22}$ & 0.000 & \underline{0.188} & 0.180 & \textbf{0.204} & 0.179 & 0.188 & 0.066 & 0.059 & 0.002 & 0.172 & 0.172 \\
 $D_{23}$ & 0.000 & 0.129 & 0.086 & 0.211 & 0.076 & 0.096 & \underline{0.214} & 0.108 & 0.000 & \textbf{0.250} & 0.160 \\
 $D_{24}$ & 0.000 & 0.208 & 0.140 & \underline{0.295} & 0.135 & 0.073 & \textbf{0.499} & 0.244 & 0.075 & 0.148 & 0.211 \\
 $D_{25}$ & 0.282 & \underline{0.488} & 0.477 & 0.471 & 0.466 & 0.444 & \textbf{0.489} & 0.436 & 0.451 & 0.470 & 0.469 \\
 $D_{26}$ & 0.000 & 0.209 & 0.142 & 0.181 & 0.134 & 0.144 & \textbf{0.222} & 0.126 & 0.025 & 0.203 & \underline{0.216} \\
 $D_{27}$ & 0.000 & 0.144 & 0.157 & \textbf{0.188} & 0.149 & 0.144 & 0.129 & 0.057 & 0.064 & \underline{0.169} & 0.164 \\
 $D_{28}$ & 0.003 & 0.285 & 0.299 & \textbf{0.349} & 0.280 & 0.304 & 0.315 & 0.205 & \underline{0.332} & 0.276 & 0.282 \\
 $D_{29}$ & 0.304 & 0.361 & 0.385 & \underline{0.454} & 0.304 & 0.430 & 0.379 & 0.178 & \textbf{0.598} & 0.355 & 0.358 \\
\midrule
R 1(2) & 0 (1) & 5 (5) & 2 (2) & 7 (5) & 0 (2) & 0 (0) & 6 (1) & 1 (0) & 2 (2) & 3 (4) & 3 (7) \\
Avg & 9.83 & \underline{4.00} & 4.78 & \underline{4.00} & 6.40 & 5.71 & 6.29 & 8.50 & 8.19 & 4.59 & \textbf{3.72} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht!]
\centering
\caption{Comparison of MCC performance across 29 imbalanced datasets for baseline (Orig) and oversampling methods: ROS, SMOTE (SM), Borderline-SMOTE (bSM), ADASYN (ADA), MWMOTE (MW), CTGAN (GAN), GAMO (GAM), MGVAE (MV), MMD, and MMD+Triplet (MMD+T). Bold and underlined values indicate best and second-best performance per dataset. R1(2): number of first(second)-place rankings; Avg: average rank across all datasets.}
\label{tab:mcc_results}
\vspace{5pt}
{\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c c c c c c c c c c c c}
\toprule
Data & Orig & ROS & SM & bSM & ADA & MW & GAN & GAM & MV & MMD & MMD+T \\
\midrule
 $D_{1}$ & 0.000 & 0.085 & 0.096 & 0.057 & \underline{0.097} & 0.047 & 0.005 & 0.006 & 0.002 & 0.097 & \textbf{0.101} \\
 $D_{2}$ & 0.165 & \textbf{0.426} & 0.395 & 0.398 & 0.363 & 0.379 & 0.258 & 0.319 & 0.376 & 0.397 & \underline{0.399} \\
 $D_{3}$ & 0.000 & 0.392 & 0.377 & 0.350 & 0.371 & 0.332 & 0.000 & 0.064 & 0.000 & \underline{0.434} & \textbf{0.452} \\
 $D_{4}$ & 0.243 & 0.697 & 0.624 & 0.585 & 0.624 & 0.678 & 0.654 & 0.367 & 0.243 & \underline{0.709} & \textbf{0.740} \\
 $D_{5}$ & 0.001 & \underline{0.138} & 0.123 & 0.128 & 0.123 & 0.121 & 0.012 & 0.036 & 0.002 & 0.122 & \textbf{0.151} \\
 $D_{6}$ & 0.590 & 0.607 & 0.616 & 0.605 & 0.587 & 0.604 & \textbf{0.632} & 0.482 & \underline{0.627} & 0.597 & 0.603 \\
 $D_{7}$ & \underline{0.822} & 0.640 & 0.722 & 0.612 & 0.637 & 0.634 & 0.698 & 0.546 & \textbf{0.837} & 0.690 & 0.688 \\
 $D_{8}$ & 0.842 & 0.863 & 0.869 & 0.863 & 0.869 & 0.864 & 0.848 & 0.870 & 0.844 & \textbf{0.884} & \underline{0.884} \\
 $D_{9}$ & 0.264 & 0.557 & 0.554 & 0.554 & 0.558 & 0.544 & \textbf{0.630} & 0.593 & 0.506 & \underline{0.605} & 0.591 \\
 $D_{10}$ & 0.639 & \textbf{0.821} & 0.774 & 0.808 & 0.805 & 0.797 & 0.540 & 0.547 & 0.786 & 0.759 & \underline{0.814} \\
 $D_{11}$ & 0.496 & 0.441 & 0.433 & \textbf{0.509} & 0.437 & 0.468 & 0.453 & 0.416 & 0.449 & 0.477 & \underline{0.497} \\
 $D_{12}$ & 0.000 & 0.331 & 0.316 & 0.319 & 0.308 & 0.306 & 0.000 & 0.011 & 0.135 & \underline{0.339} & \textbf{0.341} \\
 $D_{13}$ & 0.457 & 0.465 & 0.464 & 0.474 & \underline{0.475} & 0.464 & 0.449 & 0.452 & 0.459 & 0.469 & \textbf{0.478} \\
 $D_{14}$ & 0.046 & \underline{0.236} & 0.213 & \textbf{0.243} & 0.212 & 0.205 & 0.042 & 0.037 & 0.144 & 0.215 & 0.234 \\
 $D_{15}$ & 0.985 & \textbf{0.987} & \underline{0.987} & 0.966 & 0.966 & 0.987 & 0.975 & 0.953 & 0.979 & 0.987 & 0.987 \\
 $D_{16}$ & 0.278 & 0.599 & \underline{0.613} & 0.586 & 0.588 & 0.600 & \textbf{0.619} & 0.203 & 0.454 & 0.593 & 0.592 \\
 $D_{17}$ & 0.007 & 0.146 & 0.111 & 0.117 & 0.114 & 0.087 & 0.126 & 0.053 & 0.006 & \textbf{0.169} & \underline{0.148} \\
 $D_{18}$ & 0.185 & 0.547 & \textbf{0.586} & 0.569 & \underline{0.570} & 0.568 & 0.509 & 0.060 & 0.433 & 0.547 & 0.537 \\
 $D_{19}$ & 0.416 & \underline{0.497} & 0.454 & 0.479 & 0.441 & 0.450 & 0.437 & 0.434 & 0.430 & 0.479 & \textbf{0.501} \\
 $D_{20}$ & 0.075 & \underline{0.258} & 0.253 & \textbf{0.259} & 0.248 & 0.219 & 0.194 & 0.125 & 0.113 & 0.234 & 0.248 \\
 $D_{21}$ & 0.000 & \underline{0.119} & 0.072 & 0.052 & 0.048 & 0.114 & 0.113 & \textbf{0.164} & 0.006 & 0.077 & 0.084 \\
 $D_{22}$ & 0.000 & \underline{0.183} & 0.175 & \textbf{0.184} & 0.175 & 0.174 & 0.054 & 0.020 & 0.007 & 0.173 & 0.172 \\
 $D_{23}$ & 0.000 & 0.124 & 0.076 & 0.205 & 0.065 & 0.084 & \underline{0.214} & 0.111 & 0.002 & \textbf{0.254} & 0.161 \\
 $D_{24}$ & 0.000 & 0.202 & 0.126 & 0.292 & 0.121 & 0.059 & \textbf{0.508} & \underline{0.305} & 0.075 & 0.154 & 0.212 \\
 $D_{25}$ & 0.311 & \textbf{0.448} & 0.430 & 0.424 & 0.422 & 0.391 & \underline{0.441} & 0.379 & 0.434 & 0.427 & 0.425 \\
 $D_{26}$ & 0.000 & 0.208 & 0.137 & 0.171 & 0.132 & 0.140 & \underline{0.211} & 0.107 & 0.002 & 0.209 & \textbf{0.226} \\
 $D_{27}$ & 0.000 & 0.111 & 0.129 & \textbf{0.161} & 0.119 & 0.107 & 0.085 & 0.008 & 0.037 & \underline{0.153} & 0.141 \\
 $D_{28}$ & 0.004 & 0.311 & 0.320 & \textbf{0.353} & 0.303 & 0.317 & 0.309 & 0.181 & \underline{0.340} & 0.316 & 0.317 \\
 $D_{29}$ & 0.337 & 0.402 & 0.420 & \underline{0.467} & 0.349 & 0.456 & 0.424 & 0.193 & \textbf{0.605} & 0.405 & 0.407 \\
\midrule
R 1(2) & 0 (1) & 4 (6) & 1 (2) & 6 (1) & 0 (3) & 0 (0) & 4 (3) & 1 (1) & 2 (2) & 3 (5) & 8 (5) \\
Avg & 9.76 & \underline{4.03} & 5.00 & 4.45 & 6.40 & 6.10 & 6.29 & 8.59 & 7.84 & 4.17 & \textbf{3.36} \\
\bottomrule
\end{tabular}
}
\end{table}

\vspace{0.5em}
\noindent \textbf{Gain over Original Baseline.} Figure \ref{fig:metric_diff_boxplot} presents boxplots of per-dataset performance differences between each method and the Original baseline for each evaluation metric. Our method achieves the largest and most stable improvements across all four metrics, with median gains outperforming other techniques. 

\begin{figure*}[ht!]
\centering
\includegraphics[width=0.95\textwidth]{Figures/metric_diff_boxplot_v2.pdf}
\caption{Performance gains per metric relative to the Original baseline. Each boxplot illustrates the distribution of these performance differences across 29 real-world datasets.}
\label{fig:metric_diff_boxplot}
\end{figure*}

\vspace{0.5em}
\noindent \textbf{Statistical Significance.} 
To evaluate whether the observed performance gains of our proposed method are statistically significant against (1) the Original baseline and (2) the second-best performing method, ROS, we conducted paired t-tests using metric values from 29 datasets. As summarized in Table \ref{tab:t_test_results}, our method achieved statistically significant improvements across all four evaluation metrics against both benchmarks. Given its clear superiority over ROS, further comparisons with other methods are unnecessary. These findings substantiate the effectiveness of our approach in imbalanced classification.

\begin{table}
\centering
\caption{P-values from paired t-tests comparing our proposed method with the Original baseline method and the ROS method (the second-best performer). Statistically significant results (\(p < 0.05\)) are highlighted in bold.}
\label{tab:t_test_results}
\vspace{5pt}
\begin{tabular}{cccc}
\toprule
Metric &  Original baseline & ROS method \\
\midrule
AUROC & $\mathbf{<0.001}$& $\mathbf{0.009} $ \\
G-mean & $\mathbf{<0.001}$& $\mathbf{0.010} $ \\
F1-score &$\mathbf{<0.001}$& $\mathbf{0.047}$  \\
MCC &$\mathbf{<0.001}$& $\mathbf{0.016} $ \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Ablation Studies} \label{subsec: ablation}

We conduct a series of ablation studies to isolate the effects of key components in the proposed framework and to validate its robustness across varying experimental conditions. These studies include analyses of the regularization parameter $\lambda$, base classifier compatibility, comparative performance across different minority sample size, and empirical computational cost under varying input dimensionality and dataset size. 

\vspace{0.5em}
\noindent \textbf{Effect of the Regularization Parameter $\lambda$.} The regularization parameter $\lambda$ controls the trade-off between global distributional alignment via MMD and local boundary-aware regularization via triplet loss (see Equation~\ref{eq: total_obj}). We investigate the effect of varying $\lambda \in \{0, 0.001, 0.005, 0.01, 0.05, 0.1\}$ on overall classification performance, reporting average results across all 29 datasets in Table~\ref{tab:lambda_ablation}. The performance improves steadily as $\lambda$ increases from $0$ to $0.01$ across all four evaluation metrics, indicating that incorporating moderate boundary-aware regularization improves the informativeness of synthetic examples. However, for $\lambda > 0.01$, performance consistently declines, suggesting that excessively emphasizing local boundary structure undermines the global distributional alignment allocated by MMD. We therefore fix $\lambda = 0.01$ for all results in Section~\ref{subsec: main_results}. While this value is not optimal for every dataset individually, it provides a consistently strong performance across evaluation metrics. 

\begin{table}[h!]
  \centering
  \caption{Effect of regularization parameter $\lambda$ on average performance (SVM base classifier). Best results for each metric are bolded.}
  \vspace{5pt}
  \begin{tabular}{c|ccccc}
    \toprule
    $\lambda$ & AUROC & G-mean & F1-score & MCC \\
    \midrule
    0     & 0.8878 & 0.7543 & 0.4998 & 0.4883 \\
    0.001 & 0.8880 & 0.7566 & 0.5009 & 0.4896 \\
    0.005 & 0.8887 & 0.7618 & 0.5014 &  0.4907 \\
    0.01  & \textbf{0.8891} & \textbf{0.7647} & \textbf{0.5036} & \textbf{0.4935} \\
    0.05  & 0.8884 & 0.7603 & 0.4937 & 0.4832 \\
    0.1   & 0.8822 & 0.7569 & 0.4792 & 0.4702 \\
    \bottomrule
  \end{tabular}
  \label{tab:lambda_ablation}
\end{table}

\vspace{0.5em}
\noindent\textbf{Base Classifiers.} In  Section \ref{subsec: main_results}, we report experiments using a SVM classifier to validate our approach. To assess the general applicability of the proposed oversampling framework, we evaluate its performance in conjunction with a range of standard classifiers: Decision Tree (DT), Random Forest (RF), $k$-Nearest Neighbors ($k$NN), and Multi-Layer Perceptron (MLP). As depicted in Figure \ref{fig:basemodels}, the method consistently delivers the highest or near-highest mean values in AUROC, G-mean, F1-score, and MCC, averaged over 29 real-world datasets, regardless of which classifier is used. The consistent performance across models demonstrates that our method generates synthetic minority samples that enhance decision boundary learning across diverse classifiers. Moreover, the approach remains both effective and stable regardless of the classifier’s complexity or architecture. This confirms the practical versatility of our framework, making it suitable for diverse real-world applications where the choice of classifier may depend on task-specific requirements or computational constraints.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/real_data_results_ver1.pdf}
    \caption{
    Average classification performance using four base classifiers—Decision Tree (DT), Random Forest (RF), $k$-Nearest Neighbors ($k$NN), and Multi-Layer Perceptron (MLP). Each bar shows the average metric achieved by a specific oversampling method over all datasets.
    %The proposed method consistently achieves the highest or near-highest scores across all classifier types and evaluation metrics.
    }
    \label{fig:basemodels}
\end{figure}

\vspace{0.5em}
\noindent\textbf{Minority Sample Size.} To assess the robustness of our proposed oversampling framework under varying levels of minority class sparsity, we compare its performance against generative approaches on a simulated dataset. The dataset is generated from standard Gaussian distributions in a 50-dimensional space, with a fixed majority class size of $2000$ samples. Minority samples are similarly generated but with a mean shift of $0.3$, and their size varies across $N_{\text{min}} \in \{50, 100, 200, 500, 1000\}$. The simulations are repeated over 10 times to ensure reliability. Figure \ref{fig:Nmins_ablation} shows the average AUROC, G-mean, F1-score, and MCC across these $N_{\text{min}}$ values for GAN, GAMO, MGVAE, and our method. The results indicate that our approach consistently achieves highest or near-highest performance across all metrics, particularly in sparse regimes where GAN-based methods exhibit instability due to limited training data for realistic sample generation. As $N_{\text{min}}$ increases, the performance gap narrows, reflecting the improved capacity of generative models to capture minority distributions with more examples, yet our method maintains a competitive edge. This shows how our framework works better in very unbalanced situations by directly mapping many examples to the few, avoiding the need for many minority samples that generative models require. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/Nmins_ablation.pdf}
    \caption{
    Average classification performance across varying $N_{\text{min}}$ with a fixed $N_{\text{maj}}$. Each bar represents the mean metric value over 10 independent runs for the generative methods. %Our proposed method outperforms GAN-based methods in sparse minority regimes. 
    }
    \label{fig:Nmins_ablation}
\end{figure}

\vspace{0.5em}

\noindent \textbf{Computational Cost.} We evaluate the computational efficiency of the proposed method relative to both classical interpolation methods and deep generative baselines under varying sample sizes. The experiments employ the same Gaussian Blob dataset described in Section \ref{sec:simulation_studies} with a fixed feature dimension of $d=20$ and sample sizes ranging from $N=100$ to $N=2000$. Tables~\ref{tab:runtime_detailed_new} and \ref{tab:runtime_summary_new} present detailed and aggregated runtime comparisons, demonstrating that our method occupies a favorable middle ground: substantially faster than deep generative models (GAN, GAMO, MGVAE) while maintaining comparable speed to classical interpolation methods (SMOTE, Borderline-SMOTE, ADASYN). Specifically, our MMD+Triplet method achieves an average speedup of $2.3\times$ over deep learning approaches while operating within $1{,}500\times$ to $2{,}900\times$ of classical methods' runtime. The results reveal that our method scales roughly linearly with sample size, consistent with the complexity of computing the MMD and triplet loss. Non-adversarial optimization circumvents the computational burden typical of GAN training, yielding practical efficiency for real-world datasets. Figure~\ref{fig:runtime_ablation} provides a visual comparison of runtime trends across methods.  

\begin{table}[ht!]
\centering
\caption{Runtime comparison (seconds) across sample sizes with fixed dimension $d=20$. Classical methods (SMOTE, Borderline-SMOTE, ADASYN) are fastest but limited to local interpolation. Deep learning methods (GAN, GAMO, MGVAE) offer rich generation but incur high computational cost. Our proposed methods achieve a practical balance, delivering superior performance (see Tables~\ref{tab:auroc_results}--\ref{tab:mcc_results}) with moderate runtime.}
\label{tab:runtime_detailed_new}
\vspace{5pt}
{\small
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{rccccccccc}
\toprule
& \multicolumn{3}{c}{Classical} & \multicolumn{3}{c}{Deep Learning} & \multicolumn{2}{c}{Ours} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-9}
$N$ & SMOTE & bSMOTE & ADASYN & GAN & GAMO & MGVAE & MMD & MMD+T \\
\midrule
100 & 0.02 & 0.01 & 0.01 & 38.89 & 91.59 & 17.68 & 7.49 & \textbf{26.26} \\
500 & 0.00 & 0.00 & 0.00 & 37.57 & 95.63 & 48.31 & 11.00 & \textbf{31.57} \\
1000 & 0.01 & 0.01 & 0.01 & 39.64 & 89.00 & 95.07 & 12.81 & \textbf{38.06} \\
2000 & 0.02 & 0.02 & 0.02 & 44.06 & 99.93 & 181.84 & 20.15 & \textbf{57.57} \\
\midrule
Mean & 0.01 & 0.01 & 0.01 & 40.04 & 94.04 & 85.73 & 12.86 & \textbf{38.37} \\
\bottomrule
\end{tabular}
}
\end{table}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.95\linewidth]{Figures/runtime_comparison_lines.pdf}
%     \caption{
%     Average runtime (seconds) comparison of generative oversampling methods across total sample size and feature dimension on the Gaussian Blob simulation dataset described in Section \ref{sec:simulation_studies}. %The proposed method shows computational efficiency than other generative approaches. 
%     }
%     \label{fig:runtime_ablation}
% \end{figure}



\section{Conclusion} \label{sec:conclusion}
In this work, we have introduced a novel oversampling framework for imbalanced classification that learns a parametric transformation mapping majority samples to the minority distribution. By integrating a kernel-based Maximum Mean Discrepancy (MMD) with a triplet loss regularizer, our approach unifies global distributional alignment and local boundary-awareness in a non-adversarial manner. To validate its effectiveness, we conducted extensive empirical evaluations on 29 real-world benchmark datasets spanning diverse domains. We compared our method against classical interpolation methods (e.g., SMOTE, ADASYN) and state-of-the-art generative models (e.g., VAE- and GAN-based oversamplers) using four standard metrics: AUROC, G-mean, F1-score, and MCC. Across all metrics, our method achieved the best average rank, with consistent first- and second-place finishes on the majority of datasets. Paired t-tests confirmed that these improvements over both the Original baseline and the next-best methods are statistically significant.

We further demonstrate robustness across four base classifiers—Decision Trees, Random Forests, k-Nearest Neighbors, and Multi-Layer Perceptrons—showing best performance when swapping underlying classifiers. Ablation studies isolate the contributions of the MMD alignment and triplet-loss regularization, revealing that each component yields substantial gains in minority boundary learning. Finally, runtime analyses verify that our approach scales approximately linearly with sample size and feature dimension, outperforming generative methods in computational efficiency.

These comprehensive results underscore the versatility, stability, and practical utility of our proposed transport-map-based oversampling framework in addressing class imbalance across a wide range of settings.

Future research directions include extending the transformation map to multi-class and multi-label imbalance scenarios; investigating adaptive regularization strategies; and integrating the framework into end-to-end classifier pipelines to jointly optimize synthetic sample generation and predictive accuracy. Furthermore, theoretical analyses to establish convergence properties and distributional guarantees under complex data modalities represent promising avenues for deepening our understanding and broadening the framework’s impact.

\section*{CRediT authorship contribution statement}
\noindent S. Cha: Conceptualization, Methodology, Software, Experiments, Writing original draft. \\
H. Kim: Supervision, Methodology, Validation, Writing–review \& editing, Funding acquisition.

\section*{Declaration of competing interest}
The authors declare that they have no known competing financial interests or personal relationships 
that could have appeared to influence the work reported in this paper.

\section*{Acknowledgments}
Suman Cha's work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (RS-2016-NR017145). Hyunjoong Kim’s work was supported by the Institute of Information \& Communications Technology Planning \& Evaluation (IITP) -- ICT Challenge and Advanced Network of HRD (ICAN) grant funded by the Ministry of Science and ICT (IITP-2023-00259934) and by the NRF grant funded by the Korean government (RS-2016-NR017145).


\vskip 0.2in
\bibliography{reference}

\end{document}
