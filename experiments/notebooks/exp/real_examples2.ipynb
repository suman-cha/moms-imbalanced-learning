{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/oldrain123/IMBALANCED_CLASSIFICATION/MOMs\")\n",
    "sys.path.append(\"/home/oldrain123/IMBALANCED_CLASSIFICATION/boost\")\n",
    "sys.path.append('/home/oldrain123/IMBALANCED_CLASSIFICATION/')\n",
    "sys.path.append('/home/oldrain123/IMBALANCED_CLASSIFICATION/SMOTE_variants/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from collections import Counter\n",
    "from moms_generate import apply_transformation\n",
    "from moms_losses import MMD_est_torch\n",
    "from boost import AdaBoostClassifier, SMOTEBoost, RUSBoost, OUBoost\n",
    "from moms_utils import set_seed\n",
    "from sklearn.svm import SVC\n",
    "from SMOTE_variants.sm_variants.oversampling.mwmote import MWMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:3\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data, categorical_indices, device, h_dim, num_layers, beta, lr, n_runs=10, n_splits=10, save_path = '/results', data_name = 'wine'):\n",
    "    \"\"\"\n",
    "    Run experiments on the provided dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame with the last column as labels.\n",
    "    - n_runs: Number of repeated experiments.\n",
    "    - n_splits: Number of splits for Stratified K-Fold.\n",
    "\n",
    "    Returns:\n",
    "    - final_results: Dictionary containing averaged metrics for all methods.\n",
    "    \"\"\"\n",
    "    X = data.iloc[:, :-1]\n",
    "    Y = np.where(data.iloc[:, -1].values == 'negative', 0, 1)  # Convert labels\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"Class distribution: {Counter(Y)}\")\n",
    "\n",
    "    # One-Hot Encoding for Categorical Features\n",
    "    if categorical_indices != None:\n",
    "        encoder = OneHotEncoder(sparse_output=False, drop=\"first\", handle_unknown=\"ignore\")\n",
    "        categorical_data = encoder.fit_transform(X.iloc[:, categorical_indices])\n",
    "        numeric_data = X.drop(X.columns[categorical_indices], axis=1).values\n",
    "        X = np.hstack((numeric_data, categorical_data))\n",
    "        \n",
    "    # Parameters\n",
    "    n_epochs, h = 2000, 1.0\n",
    "\n",
    "\n",
    "    # Initialize final results storage\n",
    "    final_results = {\n",
    "        method: {\"AUC\": [], \"G-Mean\": [], \"MCC\": [], \"F1-score\": []}\n",
    "        for method in [\"AdaBoost\", \"SMOTEBoost\", \"RUSBoost\", \"OUBoost\", \"SVM\", \"SMOTE\", \"ADASYN\", \"bSMOTE\", \"ROS\", \"MWMOTE\", \"Trans(Direct)\"]\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=1203 + run, shuffle=True)\n",
    "        print(f\"\\nStarting experiment {run + 1}/{n_runs}\")\n",
    "        results = {\n",
    "            method: {\"AUC\": [], \"G-Mean\": [], \"MCC\": [], \"F1-score\": []}\n",
    "            for method in [\"AdaBoost\", \"SMOTEBoost\", \"RUSBoost\", \"OUBoost\", \"SVM\", \"SMOTE\", \"ADASYN\", \"bSMOTE\", \"ROS\", \"MWMOTE\", \"Trans(Direct)\"]\n",
    "        }\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X, Y)):\n",
    "            # if fold < 5:\n",
    "            #     continue\n",
    "            print(f\"  Fold {fold + 1}/{n_splits} - Experiment {run + 1}/{n_runs}\")\n",
    "            seed = 1203 + fold + 10 * run\n",
    "            set_seed(seed)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            # print(X_train)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            X_train = np.ascontiguousarray(X_train, dtype=np.float64)\n",
    "            X_test = np.ascontiguousarray(X_test, dtype=np.float64)\n",
    "\n",
    "            X_maj = X_train[Y_train == 0]\n",
    "            X_min = X_train[Y_train == 1]\n",
    "\n",
    "            input_dim = X_train.shape[1]\n",
    "\n",
    "            # Apply transformations\n",
    "            X_maj_direct, X_min_direct, X_trans_direct = apply_transformation(\n",
    "                X_maj,\n",
    "                X_min,\n",
    "                in_dim=input_dim,\n",
    "                h_dim=h_dim,\n",
    "                num_layers=num_layers,\n",
    "                loss_fn=MMD_est_torch,\n",
    "                device=device,\n",
    "                method='direct',\n",
    "                # selection=\"overlap\",\n",
    "                n_epochs=n_epochs,\n",
    "                h=h,\n",
    "                beta=beta,\n",
    "                lr=lr,\n",
    "                seed=seed,\n",
    "                batch_size=128,\n",
    "                k=3,\n",
    "                undersample=False\n",
    "            )\n",
    "\n",
    "            datasets = {\n",
    "                \"SVM\": (X_train, Y_train),\n",
    "                \"Boost\": (X_train, Y_train),\n",
    "                \"SMOTE\": SMOTE(random_state=seed).fit_resample(X_train, Y_train),\n",
    "                \"ADASYN\": ADASYN(random_state=seed).fit_resample(X_train, Y_train),\n",
    "                \"bSMOTE\": BorderlineSMOTE(random_state=seed).fit_resample(X_train, Y_train),\n",
    "                \"ROS\": RandomOverSampler(random_state=seed).fit_resample(X_train, Y_train),\n",
    "                \"MWMOTE\": MWMOTE(random_state=seed).sample(X_train, Y_train),\n",
    "                \"Trans(Direct)\": (\n",
    "                    np.vstack((X_maj_direct, X_min_direct, X_trans_direct)),\n",
    "                    np.hstack((np.zeros(len(X_maj_direct)), np.ones(len(X_min_direct) + len(X_trans_direct)))),\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            for method, (X_resampled, Y_resampled) in datasets.items():\n",
    "                if method == \"Boost\":\n",
    "                    # AdaBoost\n",
    "                    model_ada = AdaBoostClassifier(\n",
    "                        DecisionTreeClassifier(max_depth=5),\n",
    "                        n_estimators=100,\n",
    "                        algorithm=\"SAMME\",\n",
    "                        learning_rate=0.1,\n",
    "                        random_state=seed,\n",
    "                    )\n",
    "                    model_ada.fit(X_resampled, Y_resampled)\n",
    "                    predictions_ada = model_ada.predict(X_test)\n",
    "                    proba_ada = model_ada.predict_proba(X_test)[:, 1]\n",
    "                    results[\"AdaBoost\"][\"AUC\"].append(roc_auc_score(Y_test, proba_ada))\n",
    "                    results[\"AdaBoost\"][\"G-Mean\"].append(\n",
    "                        np.mean(geometric_mean_score(Y_test, predictions_ada, average=None))\n",
    "                    )\n",
    "                    results[\"AdaBoost\"][\"MCC\"].append(matthews_corrcoef(Y_test, predictions_ada))\n",
    "                    results['AdaBoost']['F1-score'].append(f1_score(Y_test, predictions_ada))\n",
    "\n",
    "                    # SMOTEBoost\n",
    "                    classification_smote = SMOTEBoost(\n",
    "                        learning_rate=0.1, n_samples=5, n_estimators=100, random_state=seed\n",
    "                    )\n",
    "                    classification_smote.fit(X_resampled, Y_resampled)\n",
    "                    y_pred_smote = classification_smote.predict(X_test)\n",
    "                    proba_smote = classification_smote.predict_proba(X_test)[:, 1]\n",
    "                    results[\"SMOTEBoost\"][\"AUC\"].append(roc_auc_score(Y_test, proba_smote))\n",
    "                    results[\"SMOTEBoost\"][\"G-Mean\"].append(\n",
    "                        np.mean(geometric_mean_score(Y_test, y_pred_smote, average=None))\n",
    "                    )\n",
    "                    results[\"SMOTEBoost\"][\"MCC\"].append(matthews_corrcoef(Y_test, y_pred_smote))\n",
    "                    results['SMOTEBoost']['F1-score'].append(f1_score(Y_test, y_pred_smote))\n",
    "\n",
    "                    # RUSBoost\n",
    "                    classification_rusboost = RUSBoost(\n",
    "                        learning_rate=0.1, n_samples=5, n_estimators=100, random_state=seed\n",
    "                    )\n",
    "                    classification_rusboost.fit(X_resampled, Y_resampled)\n",
    "                    y_pred_rus = classification_rusboost.predict(X_test)\n",
    "                    proba_rus = classification_rusboost.predict_proba(X_test)[:, 1]\n",
    "                    results[\"RUSBoost\"][\"AUC\"].append(roc_auc_score(Y_test, proba_rus))\n",
    "                    results[\"RUSBoost\"][\"G-Mean\"].append(\n",
    "                        np.mean(geometric_mean_score(Y_test, y_pred_rus, average=None))\n",
    "                    )\n",
    "                    results[\"RUSBoost\"][\"MCC\"].append(matthews_corrcoef(Y_test, y_pred_rus))\n",
    "                    results['RUSBoost']['F1-score'].append(f1_score(Y_test, y_pred_rus))\n",
    "\n",
    "                    # OUBoost\n",
    "                    classification_ouboost = OUBoost(\n",
    "                        learning_rate=0.1, n_samples=5, n_estimators=100, random_state=seed\n",
    "                    )\n",
    "                    classification_ouboost.fit(X_resampled, Y_resampled)\n",
    "                    y_pred_ouboost = classification_ouboost.predict(X_test)\n",
    "                    proba_ouboost = classification_ouboost.predict_proba(X_test)[:, 1]\n",
    "                    results[\"OUBoost\"][\"AUC\"].append(roc_auc_score(Y_test, proba_ouboost))\n",
    "                    results[\"OUBoost\"][\"G-Mean\"].append(\n",
    "                        np.mean(geometric_mean_score(Y_test, y_pred_ouboost, average=None))\n",
    "                    )\n",
    "                    results[\"OUBoost\"][\"MCC\"].append(matthews_corrcoef(Y_test, y_pred_ouboost))\n",
    "                    results['OUBoost']['F1-score'].append(f1_score(Y_test, y_pred_ouboost))\n",
    "                else:\n",
    "                    svm = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "                    svm.fit(X_resampled, Y_resampled)\n",
    "\n",
    "                    # Predict on the test data\n",
    "                    y_pred = svm.predict(X_test)\n",
    "                    y_pred_prob = svm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                    # xgb = XGBClassifier(n_estimators=100, max_depth=5, random_state=1203)\n",
    "                    # xgb.fit(X_resampled, Y_resampled)\n",
    "                    # y_pred = xgb.predict(X_test)\n",
    "                    # y_pred_prob = xgb.predict_proba(X_test)[:, 1]\n",
    "                    # Calculate performance metrics\n",
    "                    results[method][\"AUC\"].append(roc_auc_score(Y_test, y_pred_prob))\n",
    "                    results[method][\"G-Mean\"].append(\n",
    "                        np.mean(geometric_mean_score(Y_test, y_pred, average=None))\n",
    "                    )\n",
    "                    results[method][\"MCC\"].append(matthews_corrcoef(Y_test, y_pred))\n",
    "                    results[method][\"F1-score\"].append(f1_score(Y_test, y_pred))\n",
    "\n",
    "            # Print fold-wise results for monitoring\n",
    "            print(f\"    Intermediate Fold Results for Fold {fold + 1}:\")\n",
    "            for method, metrics in results.items():\n",
    "                print(f\"      {method}: AUC = {np.mean(metrics['AUC']):.4f}, \"\n",
    "                      f\"G-Mean = {np.mean(metrics['G-Mean']):.4f}, \"\n",
    "                      f\"MCC = {np.mean(metrics['MCC']):.4f}, \"\n",
    "                      f\"F1-score = {np.mean(metrics['F1-score']):.4f}\")\n",
    "            \n",
    "            # Fold별 변환 데이터에 대해 t-SNE 시각화\n",
    "            plot_tsne(X_train, X_trans_direct, Y_train, fold, \"Trans(Direct)\", save_path)\n",
    "\n",
    "        # Aggregate results across folds for this experiment\n",
    "        for method, metrics in results.items():\n",
    "            for metric, values in metrics.items():\n",
    "                final_results[method][metric].append(np.round(np.mean(values), 4))\n",
    "\n",
    "    # Print final averaged results\n",
    "    print(\"\\nFinal Averaged Results Across Experiments:\")\n",
    "    for method, metrics in final_results.items():\n",
    "        print(f\"  {method}:\")\n",
    "        for metric, values in metrics.items():\n",
    "            print(f\"    {metric}: {np.mean(values):.4f}\")\n",
    "\n",
    "    # Convert final_results to a pandas DataFrame\n",
    "    result_data = {\n",
    "        \"Method\": [],\n",
    "        \"Metric\": [],\n",
    "        \"Value\": [],\n",
    "    }\n",
    "\n",
    "    for method, metrics in final_results.items():\n",
    "        for metric, values in metrics.items():\n",
    "            avg_value = np.mean(values) if values else \"N/A\"\n",
    "            result_data[\"Method\"].append(method)\n",
    "            result_data[\"Metric\"].append(metric)\n",
    "            result_data[\"Value\"].append(avg_value)\n",
    "\n",
    "    results_df = pd.DataFrame(result_data)\n",
    "\n",
    "    # Save results as CSV\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_file = os.path.join(save_path, f\"{data_name}_final_results.csv\")\n",
    "    results_df.to_csv(save_file, index=False)\n",
    "    print(f\"\\nFinal results saved to {save_file}\")\n",
    "    \n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE 시각화 함수\n",
    "def plot_tsne(X_original, X_trans, Y_original, fold, method, save_path):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_combined = np.vstack((X_original, X_trans))\n",
    "    Y_combined = np.hstack((Y_original, np.full(len(X_trans), 2)))  # 변환된 샘플을 별도 레이블(2)로 지정\n",
    "\n",
    "    X_embedded = tsne.fit_transform(X_combined)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_embedded[Y_combined == 0, 0], X_embedded[Y_combined == 0, 1], label=\"Majority\", alpha=0.5)\n",
    "    plt.scatter(X_embedded[Y_combined == 1, 0], X_embedded[Y_combined == 1, 1], label=\"Minority\", alpha=0.5)\n",
    "    plt.scatter(X_embedded[Y_combined == 2, 0], X_embedded[Y_combined == 2, 1], label=\"Transformed\", alpha=0.7, marker='x', c='red')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f\"t-SNE Visualization (Fold {fold+1}, {method})\")\n",
    "    plt.xlabel(\"t-SNE Dim 1\")\n",
    "    plt.ylabel(\"t-SNE Dim 2\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_path, f\"tSNE_Fold{fold+1}_{method}.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save path \n",
    "save_path = \"/data4/oldrain123/oldrain123/results/real_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone9-18\n",
    "data_name = 'abalone9-18'\n",
    "dataframe = pd.read_csv(f\"/data4/oldrain123/oldrain123/dataset/{data_name}.dat\")\n",
    "results = run_experiment(dataframe, categorical_indices=[0], device=device, h_dim=64, num_layers=10, beta=0.1, lr = 0.001, save_path=save_path, data_name = data_name)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"  {metric}: {mean_value:.4f} ± {std_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone19\n",
    "data_name = 'abalone19'\n",
    "dataframe = pd.read_csv(f\"/data4/oldrain123/oldrain123/dataset/{data_name}.dat\")\n",
    "results = run_experiment(dataframe, categorical_indices=[0], device=device, h_dim=256, num_layers=10, beta=0.1, lr = 0.01, save_path=save_path, data_name = data_name)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"  {metric}: {mean_value:.4f} ± {std_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ionosphere\n",
    "data_name = 'ionosphere'\n",
    "dataframe = pd.read_csv(f\"/data4/oldrain123/oldrain123/dataset/{data_name}.data\")\n",
    "results = run_experiment(dataframe, categorical_indices=[], device=device, h_dim=256, num_layers=10, beta=0.1, lr = 0.1, save_path=save_path, data_name = data_name)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"  {metric}: {mean_value:.4f} ± {std_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (335, 7)\n",
      "Class distribution: Counter({np.int64(0): 300, np.int64(1): 35})\n",
      "\n",
      "Starting experiment 1/10\n",
      "  Fold 1/10 - Experiment 1/10\n",
      "Epoch 100/2000, Avg Loss: 0.09307, Reg Loss: 0.12153\n",
      "Epoch 200/2000, Avg Loss: 0.09030, Reg Loss: 0.10551\n",
      "Epoch 300/2000, Avg Loss: 0.09192, Reg Loss: 0.12006\n",
      "Epoch 400/2000, Avg Loss: 0.08843, Reg Loss: 0.11127\n",
      "Epoch 500/2000, Avg Loss: 0.08948, Reg Loss: 0.10938\n",
      "Epoch 600/2000, Avg Loss: 0.08902, Reg Loss: 0.11043\n",
      "Epoch 700/2000, Avg Loss: 0.08964, Reg Loss: 0.11260\n",
      "Epoch 800/2000, Avg Loss: 0.08972, Reg Loss: 0.11226\n",
      "Epoch 900/2000, Avg Loss: 0.08806, Reg Loss: 0.11102\n",
      "Epoch 1000/2000, Avg Loss: 0.08894, Reg Loss: 0.11074\n",
      "Epoch 1100/2000, Avg Loss: 0.08767, Reg Loss: 0.10184\n",
      "Epoch 1200/2000, Avg Loss: 0.08810, Reg Loss: 0.10963\n",
      "Epoch 1300/2000, Avg Loss: 0.09044, Reg Loss: 0.11260\n",
      "Epoch 1400/2000, Avg Loss: 0.08799, Reg Loss: 0.10812\n",
      "Epoch 1500/2000, Avg Loss: 0.08762, Reg Loss: 0.10545\n",
      "Epoch 1600/2000, Avg Loss: 0.08975, Reg Loss: 0.10914\n",
      "Epoch 1700/2000, Avg Loss: 0.09172, Reg Loss: 0.11639\n",
      "Epoch 1800/2000, Avg Loss: 0.08956, Reg Loss: 0.11055\n",
      "Epoch 1900/2000, Avg Loss: 0.08753, Reg Loss: 0.10783\n",
      "Epoch 2000/2000, Avg Loss: 0.08912, Reg Loss: 0.11332\n",
      "fit BaseWeightBoosting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oldrain123/anaconda3/envs/imb_clf/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_proba\n",
      "_compute_proba_from_decision\n",
      "    Intermediate Fold Results for Fold 1:\n",
      "      AdaBoost: AUC = 0.9000, G-Mean = 0.4916, MCC = 0.2967, F1-score = 0.3333\n",
      "      SMOTEBoost: AUC = 0.9833, G-Mean = 0.8515, MCC = 0.7167, F1-score = 0.7500\n",
      "      RUSBoost: AUC = 0.9417, G-Mean = 0.8367, MCC = 0.4641, F1-score = 0.4706\n",
      "      OUBoost: AUC = 0.9833, G-Mean = 0.7071, MCC = 0.6847, F1-score = 0.6667\n",
      "      SVM: AUC = 0.9833, G-Mean = 0.8515, MCC = 0.7167, F1-score = 0.7500\n",
      "      SMOTE: AUC = 0.9750, G-Mean = 0.9487, MCC = 0.7171, F1-score = 0.7273\n",
      "      ADASYN: AUC = 0.9667, G-Mean = 0.9487, MCC = 0.7171, F1-score = 0.7273\n",
      "      bSMOTE: AUC = 0.9750, G-Mean = 0.9487, MCC = 0.7171, F1-score = 0.7273\n",
      "      ROS: AUC = 0.9750, G-Mean = 0.9487, MCC = 0.7171, F1-score = 0.7273\n",
      "      MWMOTE: AUC = 0.9750, G-Mean = 0.9487, MCC = 0.7171, F1-score = 0.7273\n",
      "      Trans(Direct): AUC = 0.9750, G-Mean = 0.9487, MCC = 0.7171, F1-score = 0.7273\n",
      "  Fold 2/10 - Experiment 1/10\n",
      "Epoch 100/2000, Avg Loss: 0.03886, Reg Loss: 0.14307\n",
      "Epoch 200/2000, Avg Loss: 0.04012, Reg Loss: 0.14338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m data_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecoli3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data4/oldrain123/oldrain123/dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, metrics \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMethod: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(data, categorical_indices, device, h_dim, num_layers, beta, lr, n_runs, n_splits, save_path, data_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m X_maj_direct, X_min_direct, X_trans_direct \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_maj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMMD_est_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdirect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# selection=\"overlap\",\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mundersample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m\"\u001b[39m: (X_train, Y_train),\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m: (X_train, Y_train),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ),\n\u001b[1;32m     97\u001b[0m }\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, (X_resampled, Y_resampled) \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/IMBALANCED_CLASSIFICATION/MOMs/moms_generate.py:614\u001b[0m, in \u001b[0;36mapply_transformation\u001b[0;34m(X_maj, X_min, in_dim, h_dim, num_layers, loss_fn, device, method, n_epochs, lr, h, beta, batch_size, seed, undersample, k)\u001b[0m\n\u001b[1;32m    611\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m select_majority_samples(X_maj, X_min, n_trans)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# Train the transformation function\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# 변환된 데이터 생성\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_samples\u001b[39m(f, X_maj_selected, X_min, n_samples, seed):\n",
      "File \u001b[0;32m~/IMBALANCED_CLASSIFICATION/MOMs/moms_train.py:247\u001b[0m, in \u001b[0;36mtrain_map\u001b[0;34m(X_maj, X_min, in_dim, h_dim, num_layers, loss_fn, device, n_epochs, lr, h, batch_size, beta, seed, loss_params)\u001b[0m\n\u001b[1;32m    245\u001b[0m loss_args \u001b[38;5;241m=\u001b[39m loss_params \u001b[38;5;28;01mif\u001b[39;00m loss_params \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    246\u001b[0m latent_mmd_loss \u001b[38;5;241m=\u001b[39m loss_fn(latent_trans, latent_min, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_args)\n\u001b[0;32m--> 247\u001b[0m feature_mmd_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_min\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# smooth_reg_loss = torch.norm(X_trans - X_maj, p=2)  # L2 Regularization\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Feature space에서 Boundary Feature Regularization 적용\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# (X_trans: 변환된 Majority sample, X_min: Minority sample, X_maj: 원본 Majority sample)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m bod_reg_loss \u001b[38;5;241m=\u001b[39m boundary_feature_regularization(X_trans, X_min, X_maj, beta\u001b[38;5;241m=\u001b[39mbeta)\n",
      "File \u001b[0;32m~/IMBALANCED_CLASSIFICATION/MOMs/moms_losses.py:88\u001b[0m, in \u001b[0;36mMMD_est_torch\u001b[0;34m(x, y, h)\u001b[0m\n\u001b[1;32m     84\u001b[0m     mmd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Kxx)\u001b[38;5;241m/\u001b[39m(n1\u001b[38;5;241m*\u001b[39mn2) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Kxy)\u001b[38;5;241m/\u001b[39m(n1\u001b[38;5;241m*\u001b[39mn2) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Kyy)\u001b[38;5;241m/\u001b[39m(n1\u001b[38;5;241m*\u001b[39mn2)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mmd\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mMMD_est_torch\u001b[39m(x, y, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Computes the Maximum Mean Discrepancy (MMD) using PyTorch tensors and a deep kernel approach.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m            MMD value between the two distributions.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     n1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ecoli3\n",
    "data_name = 'ecoli3'\n",
    "dataframe = pd.read_csv(f\"/data4/oldrain123/oldrain123/dataset/{data_name}.dat\")\n",
    "results = run_experiment(dataframe, categorical_indices=[], device=device, h_dim=8, num_layers=4, beta=0.1, lr = 0.1, save_path=save_path, data_name = data_name)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"  {metric}: {mean_value:.4f} ± {std_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wisconsin\n",
    "data_name = 'wisconsin'\n",
    "dataframe = pd.read_csv(f\"/data4/oldrain123/oldrain123/dataset/{data_name}.dat\")\n",
    "results = run_experiment(dataframe, categorical_indices=[], device=device, h_dim=256, num_layers=10, beta=0.5, lr = 0.01, save_path=save_path, data_name = data_name)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"  {metric}: {mean_value:.4f} ± {std_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleveland-0_vs_4\n",
    "data_name = 'cleveland-0_vs_4'\n",
    "dataframe = pd.read_csv(f\"/data4/oldrain123/oldrain123/dataset/{data_name}.dat\")\n",
    "results = run_experiment(dataframe, categorical_indices=[], device=\"cpu\", h_dim=256, num_layers=10, beta=0.5, lr = 0.01, save_path=save_path, data_name = data_name)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for metric, values in metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"  {metric}: {mean_value:.4f} ± {std_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imb_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
