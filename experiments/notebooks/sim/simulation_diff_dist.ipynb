{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moms_losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoms_losses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MMD_est_torch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoms_generate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transform\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoms_visualize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualize_samples\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'moms_losses'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from moms_losses import MMD_est_torch\n",
    "from moms_generate import transform\n",
    "from moms_visualize import visualize_samples\n",
    "from moms_metrics import Metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device = \"cuda\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generate_datasets function with swapped roles of major and minor samples\n",
    "def generate_datasets(n_samples, imbalance_ratio, dataset_type=\"gaussian_vs_ring\", random_state=1203):\n",
    "    np.random.seed(random_state)\n",
    "    if dataset_type == \"gaussian_vs_ring\":\n",
    "        # gaussian : majority samples\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major\n",
    "        X_major = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_major)\n",
    "        \n",
    "        # ring : minority samples\n",
    "        angles = np.random.uniform(0, 2 * np.pi, n_minor)\n",
    "        radii = 2 + np.random.uniform(-0.5, 0.5, n_minor)\n",
    "        X_minor = np.stack([radii * np.cos(angles), radii * np.sin(angles)], axis=1)\n",
    "    elif dataset_type == \"gaussian_vs_exponential\":\n",
    "        # gaussian : majority samples\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major   \n",
    "        X_major = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_major)\n",
    "\n",
    "        # exponential : minority sasmples\n",
    "        X_minor = np.random.exponential(scale=1.0, size=(n_minor, 2))\n",
    "    elif dataset_type == \"xo\":\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major\n",
    "        # Generate XO-shaped clusters for minority\n",
    "        minor_clusters = 2\n",
    "        X_minor = []\n",
    "        for i in range(minor_clusters):\n",
    "            center = np.array([(-5 if i == 0 else 5), (-5 if i == 0 else 5)])\n",
    "            X_minor.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], n_minor // minor_clusters)\n",
    "            )\n",
    "        X_minor = np.vstack(X_minor)\n",
    "\n",
    "        # Generate XO-shaped clusters for majority\n",
    "        major_clusters = 2\n",
    "        X_major = []\n",
    "        for i in range(major_clusters):\n",
    "            center = np.array([(3 if i == 0 else -3), (-3 if i == 0 else 3)])\n",
    "            X_major.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], n_major // major_clusters)\n",
    "            )\n",
    "        X_major = np.vstack(X_major)\n",
    "    elif dataset_type == \"clustered_minority\":\n",
    "\n",
    "        # Majority samples follow a 2d uniform distribution\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major\n",
    "\n",
    "        X_major = np.random.uniform(-4, 4, size=(n_major, 2))\n",
    "\n",
    "        # Minority samples form clusters with different sample sizes\n",
    "        cluster_sizes = [n_minor // 3, n_minor // 4, n_minor - (n_minor // 3) - (n_minor // 4) - (n_minor // 5), n_minor//5]\n",
    "        cluster_centers = [\n",
    "            np.array([-4, 4]),  # Left top\n",
    "            np.array([4, -4]),  # Right bottom\n",
    "            np.array([-4, -4]),  # Left bottom\n",
    "            np.array([4, 4])\n",
    "        ]\n",
    "\n",
    "        X_minor = []\n",
    "        for i, cluster_size in enumerate(cluster_sizes):\n",
    "            center = cluster_centers[i]\n",
    "            X_minor.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], cluster_size)\n",
    "            )\n",
    "\n",
    "        X_minor = np.vstack(X_minor)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset type.\")\n",
    "    \n",
    "    labels = np.hstack((np.zeros(len(X_major)), np.ones(len(X_minor))))\n",
    "    return X_major, X_minor, labels\n",
    "\n",
    "# Define the generate_datasets function with swapped roles of major and minor samples\n",
    "# def generate_datasets(n_samples, imbalance_ratio, dataset_type=\"gaussian_vs_ring\", random_state=1203):\n",
    "    np.random.seed(random_state)\n",
    "    if dataset_type == \"gaussian_vs_ring\":\n",
    "        # gaussian : majority samples\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major\n",
    "        X_major = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_major)\n",
    "\n",
    "        # Add outliers to the majority samples\n",
    "        n_outliers = max(1, n_major // 20)  # Define outliers as 5% of the majority samples\n",
    "        outliers = np.random.uniform(-10, 10, size=(n_outliers, 2))\n",
    "        X_major = np.vstack([X_major, outliers])\n",
    "\n",
    "        # ring : minority samples\n",
    "        angles = np.random.uniform(0, 2 * np.pi, n_minor)\n",
    "        radii = 2 + np.random.uniform(-0.5, 0.5, n_minor)\n",
    "        X_minor = np.stack([radii * np.cos(angles), radii * np.sin(angles)], axis=1)\n",
    "    elif dataset_type == \"gaussian_vs_exponential\":\n",
    "        # gaussian : majority samples\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major   \n",
    "        X_major = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_major)\n",
    "\n",
    "        # Add outliers to the majority samples\n",
    "        n_outliers = max(1, n_major // 20)\n",
    "        outliers = np.random.uniform(-10, 10, size=(n_outliers, 2))\n",
    "        X_major = np.vstack([X_major, outliers])\n",
    "\n",
    "        # exponential : minority samples\n",
    "        X_minor = np.random.exponential(scale=1.0, size=(n_minor, 2))\n",
    "    elif dataset_type == \"xo\":\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major\n",
    "        # Generate XO-shaped clusters for minority\n",
    "        minor_clusters = 2\n",
    "        X_minor = []\n",
    "        for i in range(minor_clusters):\n",
    "            center = np.array([(-5 if i == 0 else 5), (-5 if i == 0 else 5)])\n",
    "            X_minor.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], n_minor // minor_clusters)\n",
    "            )\n",
    "        X_minor = np.vstack(X_minor)\n",
    "\n",
    "        # Generate XO-shaped clusters for majority\n",
    "        major_clusters = 2\n",
    "        X_major = []\n",
    "        for i in range(major_clusters):\n",
    "            center = np.array([(3 if i == 0 else -3), (-3 if i == 0 else 3)])\n",
    "            X_major.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], n_major // major_clusters)\n",
    "            )\n",
    "        X_major = np.vstack(X_major)\n",
    "\n",
    "        # Add outliers to the majority samples\n",
    "        n_outliers = max(1, n_major // 20)\n",
    "        outliers = np.random.uniform(-10, 10, size=(n_outliers, 2))\n",
    "        X_major = np.vstack([X_major, outliers])\n",
    "    elif dataset_type == \"clustered_minority\":\n",
    "\n",
    "        # Majority samples follow a 2d uniform distribution\n",
    "        n_major = round(n_samples * imbalance_ratio / (1 + imbalance_ratio))\n",
    "        n_minor = n_samples - n_major\n",
    "\n",
    "        X_major = np.random.uniform(-4, 4, size=(n_major, 2))\n",
    "\n",
    "        # Add outliers to the majority samples\n",
    "        n_outliers = max(1, n_major // 20)\n",
    "        outliers = np.random.uniform(-10, 10, size=(n_outliers, 2))\n",
    "        X_major = np.vstack([X_major, outliers])\n",
    "\n",
    "        # Minority samples form clusters with different sample sizes\n",
    "        cluster_sizes = [n_minor // 3, n_minor // 4, n_minor - (n_minor // 3) - (n_minor // 4) - (n_minor // 5), n_minor//5]\n",
    "        cluster_centers = [\n",
    "            np.array([-4, 4]),  # Left top\n",
    "            np.array([4, -4]),  # Right bottom\n",
    "            np.array([-4, -4]),  # Left bottom\n",
    "            np.array([4, 4])\n",
    "        ]\n",
    "\n",
    "        X_minor = []\n",
    "        for i, cluster_size in enumerate(cluster_sizes):\n",
    "            center = cluster_centers[i]\n",
    "            X_minor.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], cluster_size)\n",
    "            )\n",
    "\n",
    "        X_minor = np.vstack(X_minor)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset type.\")\n",
    "    \n",
    "    labels = np.hstack((np.zeros(len(X_major)), np.ones(len(X_minor))))\n",
    "    return X_major, X_minor, labels\n",
    "\n",
    "\n",
    "# parameters\n",
    "input_dim = 2 \n",
    "bw = 1.0 \n",
    "imb_ratio = 30\n",
    "n_train = 2000  # Total sample size \n",
    "n_epochs = 2000\n",
    "lr = 0.01\n",
    "eps = 0.2\n",
    "n_test = n_train//10\n",
    "te_imb_ratio = imb_ratio\n",
    "n_runs = 1\n",
    "bs = n_train\n",
    "seed = 1203\n",
    "\n",
    "# Dataset types to visualize\n",
    "dataset_types = [\"gaussian_vs_ring\", \"gaussian_vs_exponential\", \"xo\", \"clustered_minority\"]\n",
    "# dataset_types = [\"xo\"]\n",
    "\n",
    "# Generate and visualize datasets\n",
    "for dataset_type in dataset_types:\n",
    "    print(f\"Visualizing {dataset_type} dataset...\")\n",
    "    X_maj, X_min, labels = generate_datasets(\n",
    "        n_samples=n_train,\n",
    "        imbalance_ratio=imb_ratio,\n",
    "        dataset_type=dataset_type,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_maj[:, 0], X_maj[:, 1], color='red', alpha=0.6, label=\"Majority\")\n",
    "    plt.scatter(X_min[:, 0], X_min[:, 1], color='blue', alpha=0.6, label=\"Minority\")\n",
    "    plt.title(f\"{dataset_type.replace('_', ' ').capitalize()} Dataset\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=1203)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=1203)\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=None, eval_metric=\"logloss\", random_state=1203)\n",
    "svm = SVC(kernel=\"rbf\", probability=True, random_state=1203)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"KNN\": knn,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"XGBoost\": xgb,\n",
    "    \"SVM\": svm,\n",
    "}\n",
    "\n",
    "dataset_types = [\"clustered_minority\", \"gaussian_vs_ring\", \"xo\", \"gaussian_vs_exponential\"]\n",
    "# dataset_types = [\"xo\"]\n",
    "methods = [\"Original\", \"SMOTE\", \"ADASYN\", \"Borderline-SMOTE\", \"Random Oversampling\", \"Trans(Direct)\", \"Trans(Noise)\"]\n",
    "te_imb_ratios = [imb_ratio]  # Define test imbalance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "X_all = torch.cat([torch.tensor(X_min), torch.tensor(X_maj)], dim=0)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Convert tensors to NumPy arrays for NearestNeighbors\n",
    "X_min_np = X_min\n",
    "X_all_np = X_all\n",
    "\n",
    "# Use NearestNeighbors to find the k nearest neighbors\n",
    "nn = NearestNeighbors(n_neighbors=k+1, algorithm='auto', n_jobs=-1)\n",
    "nn.fit(X_all_np)\n",
    "_, nn_idx = nn.kneighbors(X_min_np)\n",
    "nn_idx = torch.tensor(nn_idx, device=X_min.device)[:, 1:]\n",
    "\n",
    "\n",
    "# Step 2: Identify DANGER set and refined DANGER set\n",
    "danger_mask = []\n",
    "refined_danger_mask = []\n",
    "for i, nn_indices in enumerate(nn_idx):\n",
    "    # Count majority neighbors among the k nearest neighbors\n",
    "    majority_count = sum(1 for idx in nn_indices if idx >= X_min.shape[0])  # Majority samples have indices >= len(X_min)\n",
    "    # Identify DANGER set: More than half of the neighbors are majority class\n",
    "    if majority_count > (k // 2):\n",
    "        danger_mask.append(i)\n",
    "        # Identify refined DANGER set: All k neighbors are majority class\n",
    "        all_majority = all(idx >= X_min.shape[0] for idx in nn_indices)\n",
    "        if all_majority:\n",
    "            refined_danger_mask.append(i)\n",
    "\n",
    "danger_set = X_min[torch.tensor(danger_mask, device=X_min.device)]\n",
    "refined_danger_set = X_min[torch.tensor(refined_danger_mask, device=X_min.device)]\n",
    "\n",
    "# Define borderline DANGER set: DANGER set minus refined DANGER set\n",
    "borderline_danger_mask = list(set(danger_mask) - set(refined_danger_mask))\n",
    "borderline_danger_set = X_min[torch.tensor(borderline_danger_mask, device=X_min.device)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_maj[:, 0], X_maj[:, 1], color='red', alpha=0.5, label='Majority')\n",
    "plt.scatter(X_min[:, 0], X_min[:, 1], color='blue', alpha=0.7, label='Minority')\n",
    "plt.scatter(danger_set[:, 0], danger_set[:, 1], color='skyblue', alpha=0.5, label='Danger Set')\n",
    "plt.scatter(borderline_danger_set[:, 0], borderline_danger_set[:, 1], color='yellow', alpha=0.7, edgecolor='black', linewidth=1, label='Borderline Set')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title(\"Borderline Set\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_type in dataset_types:\n",
    "    print(f\"Processing dataset type: {dataset_type}\")\n",
    "    \n",
    "    results = {\n",
    "        method: {model_name: {\"G-mean\": [], \"F1-score\": [], \"MCC\": [], \"Accuracy\": [], \"AUROC\": []} for model_name in models.keys()}\n",
    "        for method in methods\n",
    "    }\n",
    "    for te_imb_ratio in te_imb_ratios:\n",
    "        print(f\"Processing experiments with test imbalance ratio: {te_imb_ratio}\")\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            print(f\"Run {run + 1}/{n_runs} - Generating {dataset_type} dataset...\")\n",
    "            seed = 1203 * te_imb_ratio + run\n",
    "            # Generate training data\n",
    "            X_tr_maj, X_tr_min, _ = generate_datasets(\n",
    "                n_samples=n_train,\n",
    "                imbalance_ratio=imb_ratio,\n",
    "                dataset_type=dataset_type,\n",
    "                random_state=seed\n",
    "            )\n",
    "            n_maj = len(X_tr_maj)\n",
    "            n_min = len(X_tr_min)\n",
    "            X_tr = np.vstack((X_tr_maj, X_tr_min))\n",
    "            tr_labels = np.hstack((np.ones(n_maj), np.zeros(n_min)))\n",
    "\n",
    "            # Generate test data with the specified imbalance ratio\n",
    "            X_te_maj, X_te_min, _ = generate_datasets(\n",
    "                n_samples=n_test,\n",
    "                imbalance_ratio=te_imb_ratio,\n",
    "                dataset_type=dataset_type,\n",
    "                random_state=seed + n_test\n",
    "            )\n",
    "            X_te = np.vstack((X_te_maj, X_te_min))\n",
    "            te_labels = np.hstack((np.ones(len(X_te_maj)), np.zeros(len(X_te_min))))\n",
    " \n",
    "            # Apply transformations\n",
    "            X_tr_maj_direct, X_tr_min_direct, X_tr_trans_direct = apply_transformation(\n",
    "                X_tr_maj,\n",
    "                X_tr_min,\n",
    "                in_dim=input_dim,\n",
    "                h_dim=256,\n",
    "                num_layers=10,\n",
    "                loss_fn=MMD_est_torch,\n",
    "                device=device,\n",
    "                method='direct',\n",
    "                selection=\"random\",\n",
    "                n_epochs=n_epochs,\n",
    "                beta=0.0,\n",
    "                lr=lr,\n",
    "                seed=seed,\n",
    "                k=5\n",
    "            )\n",
    "\n",
    "            X_tr_maj_noise, X_tr_min_noise, X_tr_trans_noise = apply_transformation(\n",
    "                X_tr_maj,\n",
    "                X_tr_min,\n",
    "                in_dim=input_dim,\n",
    "                h_dim=256,\n",
    "                num_layers=10,\n",
    "                loss_fn=MMD_est_torch,\n",
    "                device=device,\n",
    "                method=\"noise\",\n",
    "                selection='random',\n",
    "                n_epochs=n_epochs,\n",
    "                beta=0.0,\n",
    "                lr=lr,\n",
    "                seed=seed,\n",
    "                k=5\n",
    "            )\n",
    "            \n",
    "\n",
    "            datasets = {\n",
    "                \"Original\": (X_tr, tr_labels),\n",
    "                \"SMOTE\": SMOTE(random_state=seed).fit_resample(X_tr, tr_labels),\n",
    "                \"ADASYN\": ADASYN(random_state=seed).fit_resample(X_tr, tr_labels),\n",
    "                \"Borderline-SMOTE\": BorderlineSMOTE(random_state=seed).fit_resample(X_tr, tr_labels),\n",
    "                \"Random Oversampling\": RandomOverSampler(random_state=seed).fit_resample(X_tr, tr_labels),\n",
    "                \"Trans(Direct)\": (np.vstack((X_tr_maj_direct, X_tr_min_direct, X_tr_trans_direct)),\n",
    "                                    np.hstack((np.ones(len(X_tr_maj_direct)), np.zeros(len(X_tr_min_direct)), np.zeros(len(X_tr_trans_direct))))),\n",
    "                \"Trans(Noise)\": (np.vstack((X_tr_maj_noise, X_tr_min_noise, X_tr_trans_noise)),\n",
    "                                    np.hstack((np.ones(len(X_tr_maj_noise)), np.zeros(len(X_tr_min_noise)), np.zeros(len(X_tr_trans_noise)))))\n",
    "            }   \n",
    "\n",
    "            # Evaluate each model and method\n",
    "            for method, (X_train, y_train) in datasets.items():\n",
    "                for model_name, model in models.items():\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                    y_pred = model.predict(X_te)\n",
    "                    y_pred_prob = model.predict_proba(X_te)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    metrics = Metrics(te_labels, y_pred, y_pred_prob)\n",
    "                    results[method][model_name][\"G-mean\"].append(metrics.g_mean())\n",
    "                    results[method][model_name][\"F1-score\"].append(metrics.f1_score())\n",
    "                    results[method][model_name][\"MCC\"].append(metrics.mcc())\n",
    "                    results[method][model_name][\"Accuracy\"].append(metrics.accuracy())\n",
    "                    if y_pred_prob is not None:\n",
    "                        results[method][model_name][\"AUROC\"].append(metrics.roc_auc())\n",
    "\n",
    "            if run >= 0:\n",
    "                # Visualize results\n",
    "                fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "                axes = axes.ravel()  # Flatten the 3x2 grid to a 1D array for easier iteration\n",
    "\n",
    "                for i, (method, (X_method, _)) in enumerate(datasets.items()):\n",
    "                    if i >= len(axes):  # Skip if there are more datasets than subplots\n",
    "                        break\n",
    "                    ax = axes[i]\n",
    "                    if method == \"Trans(Direct)\":\n",
    "                        generated_samples = X_method[len(X_tr_maj_direct) + len(X_tr_min_direct):]\n",
    "                        visualize_samples(ax, X_tr_maj_direct, X_tr_min_direct, generated_samples, f\"{method} - {dataset_type}\")\n",
    "                    elif method == \"Trans(Noise)\":\n",
    "                        generated_samples_noise = X_method[len(X_tr_maj_noise) + len(X_tr_min_noise):]\n",
    "                        visualize_samples(ax, X_tr_maj_direct, X_tr_min_noise, generated_samples_noise, f\"{method} - {dataset_type}\")\n",
    "                    else:\n",
    "                        generated_samples = X_method[len(X_tr):]\n",
    "                        visualize_samples(ax, X_tr_maj, X_tr_min, generated_samples, f\"{method} - {dataset_type}\")\n",
    "\n",
    "                # Adjust layout and display the plot\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                    \n",
    "        # Summarize results\n",
    "        result_summary = {}\n",
    "        for method, method_results in results.items():\n",
    "            summary = {}\n",
    "            for model_name, metrics in method_results.items():\n",
    "                for metric_name, values in metrics.items():\n",
    "                    # Compute the average metric across runs\n",
    "                    avg_value = np.round(np.mean(values), 4) if values else \"Not available\"\n",
    "                    summary[f\"{model_name}_{metric_name}\"] = avg_value\n",
    "            result_summary[method] = summary\n",
    "\n",
    "        # Convert result_summary to a DataFrame\n",
    "        df = pd.DataFrame.from_dict(result_summary, orient=\"index\")\n",
    "\n",
    "        # Save to CSV\n",
    "        output_path = f\"results/simulation/{dataset_type}_te_imb_{te_imb_ratio}_results.csv\"\n",
    "        df.to_csv(output_path, index=True)\n",
    "        print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_tr_trans_direct).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "    [0, 101, 102, 200, 201],  # Sample 0\n",
    "    [1, 100, 201, 202, 200],  # Sample 1\n",
    "    [2, 200, 201, 202, 100],  # Sample 2\n",
    "]\n",
    "\n",
    "maj_indices = [100, 101, 102]\n",
    "trans_indices = [200, 201, 202]\n",
    "\n",
    "filtered_trans_samples = []\n",
    "for idx, neighbors in enumerate(indices):\n",
    "    neighbor_set = set(neighbors[1:])\n",
    "    majority_count = sum(1 for neighbor in neighbor_set if neighbor in maj_indices)\n",
    "    transformed_count = sum(1 for neighbor in neighbor_set if neighbor in trans_indices)\n",
    "    if majority_count <= 1 and transformed_count != len(neighbor_set):\n",
    "        filtered_trans_samples.append(idx)\n",
    "\n",
    "print(\"Filtered indices:\", filtered_trans_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "combined_data = np.vstack([X_tr_min, X_tr_maj, X_tr_trans_direct])  # Include existing X_trans samples\n",
    "nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='auto').fit(combined_data)\n",
    "_, indices = nbrs.kneighbors(X_tr_trans_direct)\n",
    "\n",
    "min_indices = np.arange(len(X_tr_min))\n",
    "maj_indices = np.arange(len(X_tr_min), len(X_tr_min) + len(X_tr_maj))\n",
    "trans_indices = np.arange(len(X_tr_min) + len(X_tr_maj), combined_data.shape[0])\n",
    "\n",
    "filtered_trans_samples = []\n",
    "for idx, neighbors in enumerate(indices):\n",
    "    neighbor_set = set(neighbors[1:])  # Exclude the sample itself\n",
    "    # Check if all neighbors are majority or X_trans samples\n",
    "    if not (neighbor_set.issubset(maj_indices) or neighbor_set.issubset(trans_indices)):\n",
    "        filtered_trans_samples.append(X_tr_trans_direct[idx])\n",
    "\n",
    "X_trans_samples = np.array(filtered_trans_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering samples\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "combined_data = np.vstack([X_tr_maj, X_tr_min, X_tr_trans_direct])\n",
    "nbrs = NearestNeighbors(n_neighbors=5 + 1, algorithm='auto').fit(combined_data)\n",
    "_, indices = nbrs.kneighbors(X_tr_trans_direct)\n",
    "\n",
    "\n",
    "# Identify minority indices in the combined data\n",
    "min_indices = np.arange(len(X_min)) + len(X_maj)  # Adjust indices for X_min in the combined dataset\n",
    "\n",
    "# Retain samples where at least one neighbor belongs to X_min\n",
    "keep_mask = np.any(np.isin(indices[:, 1:], min_indices), axis=1)\n",
    "X_trans_samples = X_tr_trans_direct[keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_tr_trans_direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_tr_maj) - len(X_tr_min))\n",
    "len(X_trans_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generate_datasets function\n",
    "def generate_datasets(n_samples, imbalance_ratio, dataset_type=\"gaussian_vs_ring\", random_state=1203):\n",
    "    np.random.seed(random_state)\n",
    "    if dataset_type == \"gaussian_vs_ring\":\n",
    "        n_major = round(n_samples * imbalance_ratio)\n",
    "        n_minor = n_samples - n_major\n",
    "        X_minor = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_minor)\n",
    "        angles = np.random.uniform(0, 2 * np.pi, n_major)\n",
    "        radii = 3 + np.random.uniform(-0.5, 0.5, n_major)\n",
    "        X_major = np.stack([radii * np.cos(angles), radii * np.sin(angles)], axis=1)\n",
    "    elif dataset_type == \"gaussian_vs_spiral\":\n",
    "        n_major = round(n_samples * imbalance_ratio)\n",
    "        n_minor = n_samples - n_major\n",
    "        X_minor = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_minor)\n",
    "        theta = np.sqrt(np.random.uniform(0, 4 * np.pi, n_major)) * 2\n",
    "        r = theta\n",
    "        X_major = np.stack([r * np.cos(theta), r * np.sin(theta)], axis=1)\n",
    "    elif dataset_type == \"gaussian_vs_exponential\":\n",
    "        n_major = round(n_samples * imbalance_ratio)\n",
    "        n_minor = n_samples - n_major\n",
    "        X_minor = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], n_minor)\n",
    "        X_major = np.random.exponential(scale=1.0, size=(n_major, 2))\n",
    "    elif dataset_type == \"clustered_gaussian\":\n",
    "        n_major = round(n_samples * imbalance_ratio)\n",
    "        n_minor = n_samples - n_major\n",
    "        # Generate XO-shaped clusters for majority\n",
    "        major_clusters = 2\n",
    "        X_major = []\n",
    "        for i in range(major_clusters):\n",
    "            center = np.array([(-5 if i == 0 else 5), (-5 if i == 0 else 5)])\n",
    "            X_major.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], n_major // major_clusters)\n",
    "            )\n",
    "        X_major = np.vstack(X_major)\n",
    "\n",
    "        # Generate XO-shaped clusters for minority\n",
    "        minor_clusters = 2\n",
    "        X_minor = []\n",
    "        for i in range(minor_clusters):\n",
    "            center = np.array([(5 if i == 0 else -5), (-5 if i == 0 else 5)])\n",
    "            X_minor.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], n_minor // minor_clusters)\n",
    "            )\n",
    "        X_minor = np.vstack(X_minor)\n",
    "    elif dataset_type == \"clustered_minority\":\n",
    "        # Majority samples follow a uniform distribution\n",
    "        n_major = round(n_samples * imbalance_ratio)\n",
    "        n_minor = n_samples - n_major\n",
    "\n",
    "        X_major = np.random.uniform(-5, 5, size=(n_major, 2))\n",
    "\n",
    "        # Minority samples form clusters with different sample sizes\n",
    "        minor_clusters = 3\n",
    "        cluster_sizes = [n_minor // 3, n_minor // 4, n_minor - (n_minor // 3) - (n_minor // 4)]\n",
    "        cluster_centers = [\n",
    "            np.array([-5, 5]),  # Left top\n",
    "            np.array([5, -5]),  # Right bottom\n",
    "            np.array([-5, -5])  # Left bottom\n",
    "        ]\n",
    "\n",
    "        X_minor = []\n",
    "        for i, cluster_size in enumerate(cluster_sizes):\n",
    "            center = cluster_centers[i]\n",
    "            X_minor.append(\n",
    "                np.random.multivariate_normal(center, [[1, 0], [0, 1]], cluster_size)\n",
    "            )\n",
    "        X_minor = np.vstack(X_minor)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset type.\")\n",
    "    \n",
    "    labels = np.hstack((np.ones(len(X_major)), np.zeros(len(X_minor))))\n",
    "    return X_major, X_minor, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=1203)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=1203)\n",
    "xgb = XGBClassifier(n_estimators=500, max_depth=10, eval_metric=\"logloss\", random_state=1203)\n",
    "svm = SVC(kernel=\"rbf\", probability=True, random_state=1203)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"KNN\": knn,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"XGBoost\": xgb,\n",
    "    \"SVM\": svm,\n",
    "}\n",
    "\n",
    "dataset_types = [\"clustered_minority\", \"clustered_gaussian\", \"gaussian_vs_ring\", \"gaussian_vs_spiral\", \"gaussian_vs_exponential\"]\n",
    "methods = [\"Original\", \"SMOTE\", \"Borderline-SMOTE\", \"Random Oversampling\", \"Trans(Direct)\", \"Trans(Sampling)\"]\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    print(f\"Processing dataset type: {dataset_type}\")\n",
    "    \n",
    "    results = {\n",
    "        method: {model_name: {\"G-mean\": [], \"F1-score\": [], \"MCC\": [], \"Accuracy\": [], \"AUROC\": []} for model_name in models.keys()}\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"Run {run + 1}/{n_runs} - Generating {dataset_type} dataset...\")\n",
    "        X_major, X_minor, labels = generate_datasets(\n",
    "            n_samples=n_train,\n",
    "            imbalance_ratio=imb_ratio,\n",
    "            dataset_type=dataset_type,\n",
    "            random_state=1203 + run\n",
    "        )\n",
    "\n",
    "        # Create test set\n",
    "        test_X_major, test_X_minor, test_labels = generate_datasets(\n",
    "            n_samples=n_test,\n",
    "            imbalance_ratio=te_imb_ratio,\n",
    "            dataset_type=dataset_type,\n",
    "            random_state=1203 + run\n",
    "        )\n",
    "        test_samples = np.vstack((test_X_major, test_X_minor))\n",
    "        test_labels = np.hstack((np.ones(len(test_X_major)), np.zeros(len(test_X_minor))))\n",
    "\n",
    "        # Apply transformations\n",
    "        directly_transformed = apply_transformation_with_drop(\n",
    "            X_major,\n",
    "            X_minor,\n",
    "            in_dim=input_dim,\n",
    "            loss_fn=MMD_est_torch,\n",
    "            device=\"cpu\",\n",
    "            method=\"direct\",\n",
    "            selection=\"random\",\n",
    "            n_epochs=epochs,\n",
    "            lr=lr,\n",
    "            seed=1203 + run\n",
    "        )\n",
    "\n",
    "        sampling_transformed = apply_transformation_with_drop(\n",
    "            X_major,\n",
    "            X_minor,\n",
    "            in_dim=input_dim,\n",
    "            loss_fn=MMD_est_torch,\n",
    "            device=\"cpu\",\n",
    "            method=\"sampling\",\n",
    "            n_epochs=epochs,\n",
    "            lr=lr,\n",
    "            seed=1203 + run\n",
    "        )\n",
    "\n",
    "        datasets = {\n",
    "            \"Original\": (np.vstack((X_major, X_minor)), np.hstack((np.ones(len(X_major)), np.zeros(len(X_minor))))),\n",
    "            \"SMOTE\": SMOTE(random_state=1203).fit_resample(np.vstack((X_major, X_minor)), np.hstack((np.ones(len(X_major)), np.zeros(len(X_minor))))),\n",
    "            # \"ADASYN\": ADASYN(random_state=1203).fit_resample(np.vstack((X_major, X_minor)), np.hstack((np.ones(len(X_major)), np.zeros(len(X_minor))))),\n",
    "            \"Borderline-SMOTE\": BorderlineSMOTE(random_state=1203).fit_resample(np.vstack((X_major, X_minor)), np.hstack((np.ones(len(X_major)), np.zeros(len(X_minor))))),\n",
    "            \"Random Oversampling\": RandomOverSampler(random_state=1203).fit_resample(np.vstack((X_major, X_minor)), np.hstack((np.ones(len(X_major)), np.zeros(len(X_minor))))),\n",
    "            \"Trans(Direct)\": (np.vstack((X_major, directly_transformed, X_minor)),\n",
    "                                np.hstack((np.ones(len(X_major)), np.zeros(len(directly_transformed)), np.zeros(len(X_minor))))),\n",
    "            \"Trans(Sampling)\": (np.vstack((X_major, sampling_transformed, X_minor)),\n",
    "                                  np.hstack((np.ones(len(X_major)), np.zeros(len(sampling_transformed)), np.zeros(len(X_minor)))))\n",
    "        }\n",
    "\n",
    "        # Evaluate each model and method\n",
    "        for method, (X_train, y_train) in datasets.items():\n",
    "            for model_name, model in models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(test_samples)\n",
    "                y_pred_prob = model.predict_proba(test_samples)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "                # Calculate metrics\n",
    "                metrics = Metrics(test_labels, y_pred, y_pred_prob)\n",
    "                results[method][model_name][\"G-mean\"].append(metrics.g_mean())\n",
    "                results[method][model_name][\"F1-score\"].append(metrics.f1_score())\n",
    "                results[method][model_name][\"MCC\"].append(metrics.mcc())\n",
    "                results[method][model_name][\"Accuracy\"].append(metrics.accuracy())\n",
    "                if y_pred_prob is not None:\n",
    "                    results[method][model_name][\"AUROC\"].append(metrics.roc_auc())\n",
    "\n",
    "    # Visualize results\n",
    "    for method, (X_method, _) in datasets.items():\n",
    "        generated_samples = X_method[len(X_major) + len(X_minor):]\n",
    "        visualize_samples(X_major, X_minor, generated_samples, f\"{method} - {dataset_type}\")\n",
    "\n",
    "    result_summary = {}\n",
    "    for method, method_results in results.items():\n",
    "        summary = {}\n",
    "        for model_name, metrics in method_results.items():\n",
    "            for metric_name, values in metrics.items():\n",
    "                # Compute the average metric across runs\n",
    "                avg_value = np.mean(values) if values else \"Not available\"\n",
    "                summary[f\"{model_name}_{metric_name}\"] = avg_value\n",
    "        result_summary[method] = summary\n",
    "\n",
    "    # Convert result_summary to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(result_summary, orient=\"index\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"results/simulation/{dataset_type}_results2.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imb_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
